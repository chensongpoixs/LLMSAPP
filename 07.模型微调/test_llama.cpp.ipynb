{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c4576ad",
   "metadata": {},
   "source": [
    "#  æµ‹è¯• llama.cpp httpæ¥å£\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c2d1c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json;\n",
    "import urllib.request;\n",
    "\n",
    "\n",
    "def chat_model(prompt, model=\"qwen3:latest\", url=\"http://192.168.9.179:11434/api/chat\"):\n",
    "    data = {\n",
    "         \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"options\": {\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0.0\n",
    "        }\n",
    "    };\n",
    "\n",
    "    payload = json.dumps(data).encode(\"utf-8\");\n",
    "    request = urllib.request.Request(url, method=\"POST\", data=payload);\n",
    "    request.add_header(\"Content-Type\", \"application/json\");\n",
    "    content = \"\";\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        while True:\n",
    "            line = response.readline().decode('utf-8');\n",
    "            if not line:\n",
    "                break;\n",
    "\n",
    "            response_json = json.loads(line);\n",
    "            # print(f\"response_json:{response_json}\");\n",
    "            content +=response_json[\"message\"][\"content\"];\n",
    "        return content;\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c903193",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ce0dfd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‚¨å¥½ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œæ˜¯é€šä¹‰å®éªŒå®¤ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå¸®åŠ©æ‚¨å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ã€é€»è¾‘æ¨ç†ã€ç¼–ç¨‹ã€å¤šè¯­è¨€ç†è§£ç­‰å¤šç§ä»»åŠ¡ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "request = chat_model(\"æ‚¨å¥½ï¼Œ ä½ æ˜¯ï¼Ÿï¼Ÿ\" ); #, url=\"http://127.0.0.1:8899/v1/chat/completions\" );\n",
    "\n",
    "print(request);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009db21",
   "metadata": {},
   "source": [
    "llama.cpp è‡ªå¸¦çš„ HTTP æœåŠ¡æä¾› OpenAI å…¼å®¹çš„ APIï¼Œè¯·æ±‚æ ¼å¼ä¸è°ƒç”¨ OpenAI å®˜æ–¹æ¥å£ç±»ä¼¼ã€‚ä»¥ä¸‹æ˜¯å„æ ¸å¿ƒç«¯ç‚¹çš„è¯·æ±‚æ ¼å¼è¯¦è§£ã€‚\n",
    "\n",
    "### ğŸ”Œ å¯åŠ¨ HTTP æœåŠ¡\n",
    "å¯åŠ¨æœåŠ¡ï¼ˆé€šå¸¸åœ¨ `localhost:8080`ï¼‰ï¼Œè¿™æ˜¯åç»­æ‰€æœ‰è¯·æ±‚çš„åŸºç¡€ï¼š\n",
    "```bash\n",
    "./llama-server --model models/ä½ çš„æ¨¡å‹.gguf\n",
    "```\n",
    "\n",
    "### ğŸ“ æ ¸å¿ƒ API ç«¯ç‚¹ä¸è¯·æ±‚æ ¼å¼\n",
    "ä¸‹è¡¨æ±‡æ€»äº†æœ€å¸¸ç”¨çš„å‡ ä¸ª HTTP API ç«¯ç‚¹ï¼š\n",
    "\n",
    "| ç«¯ç‚¹ | æ–¹æ³• | ä¸»è¦åŠŸèƒ½ | å…³é”®è¯·æ±‚ä½“/å‚æ•° | ä¸»è¦è¿”å›å­—æ®µ |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **`/completion`** | POST | **æ–‡æœ¬è¡¥å…¨** | `prompt`, `stream`, `temperature`, `max_tokens` | `content` |\n",
    "| **`/v1/chat/completions`** | POST | **å¯¹è¯è¡¥å…¨** (ä¸»æµ) | `model`, `messages`, `stream`, `temperature` | `choices[0].message.content` |\n",
    "| **`/tokenize`** | POST | æ–‡æœ¬è½¬ Token ID | `content` | `tokens` |\n",
    "| **`/embedding`** | POST | è·å–æ–‡æœ¬å‘é‡ | `content` | `embedding` |\n",
    "| **`/health`** | GET | å¥åº·æ£€æŸ¥ | æ—  | æœåŠ¡çŠ¶æ€ä¿¡æ¯ |\n",
    "\n",
    "### ğŸ’¬ è¯¦ç»†è¯·æ±‚ç¤ºä¾‹\n",
    "\n",
    "#### 1. åŸºç¡€æ–‡æœ¬è¡¥å…¨ (`/completion`)\n",
    "è¿™æ˜¯è¾ƒæ—©æœŸçš„ç«¯ç‚¹ï¼Œé€‚ç”¨äºç®€å•çš„ç»­å†™ä»»åŠ¡ã€‚\n",
    "```bash\n",
    "curl http://localhost:8080/completion -H \"Content-Type: application/json\" -d '{\n",
    "  \"prompt\": \"æ³•å›½çš„é¦–éƒ½æ˜¯\",\n",
    "  \"stream\": false,\n",
    "  \"temperature\": 0.8,\n",
    "  \"max_tokens\": 50\n",
    "}'\n",
    "```\n",
    "\n",
    "#### 2. å¯¹è¯è¡¥å…¨ (`/v1/chat/completions`)\n",
    "**è¿™æ˜¯æœ€å¸¸ç”¨ã€åŠŸèƒ½æœ€ä¸°å¯Œçš„ç«¯ç‚¹**ï¼Œå…¶è¯·æ±‚ä½“æ ¼å¼å®Œå…¨éµå¾ª OpenAI æ ‡å‡†ã€‚\n",
    "```bash\n",
    "curl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n",
    "  \"model\": \"gpt-3.5-turbo\", // æ¨¡å‹åï¼Œå¯è‡ªå®šä¹‰\n",
    "  \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"è¯·è§£é‡Šä»€ä¹ˆæ˜¯çŸ¥è¯†è’¸é¦ã€‚\"}\n",
    "  ],\n",
    "  \"stream\": false,\n",
    "  \"temperature\": 0.7,\n",
    "  \"max_tokens\": 500\n",
    "}'\n",
    "```\n",
    "\n",
    "#### 3. æµå¼å“åº”\n",
    "å°† `\"stream\": true`ï¼Œæ•°æ®ä¼šä»¥ Server-Sent Events (SSE) æ ¼å¼æµå¼è¿”å›ã€‚\n",
    "```bash\n",
    "curl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n",
    "  \"model\": \"gpt-3.5-turbo\",\n",
    "  \"messages\": [{\"role\": \"user\", \"content\": \"è¯·é€å­—èƒŒè¯µã€Šé™å¤œæ€ã€‹\"}],\n",
    "  \"stream\": true\n",
    "}'\n",
    "```\n",
    "\n",
    "#### 4. å‡½æ•°è°ƒç”¨ï¼ˆé«˜çº§åŠŸèƒ½ï¼‰\n",
    "llama.cpp æ”¯æŒå‡½æ•°è°ƒç”¨ï¼ˆéœ€ä½¿ç”¨æ”¯æŒæ­¤åŠŸèƒ½çš„æ¨¡å‹å¹¶æ­£ç¡®é…ç½®æ¨¡æ¿ï¼‰ã€‚åœ¨ `/v1/chat/completions` è¯·æ±‚çš„ `tools` å‚æ•°ä¸­å®šä¹‰å·¥å…·ï¼Œæ¨¡å‹ä¼šåœ¨éœ€è¦æ—¶è¿”å›å·¥å…·è°ƒç”¨è¯·æ±‚ã€‚\n",
    "```bash\n",
    "curl http://localhost:8080/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n",
    "  \"model\": \"gpt-3.5-turbo\",\n",
    "  \"messages\": [{\"role\": \"user\", \"content\": \"åŒ—äº¬ç°åœ¨çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\"}],\n",
    "  \"tools\": [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "      \"name\": \"get_current_weather\",\n",
    "      \"description\": \"è·å–æŒ‡å®šåŸå¸‚çš„å½“å‰å¤©æ°”\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"location\": {\"type\": \"string\", \"description\": \"åŸå¸‚å\"}\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "      }\n",
    "    }\n",
    "  }]\n",
    "}'\n",
    "```\n",
    "æˆåŠŸè°ƒç”¨åï¼Œè¿”å›çš„ `message` ä¸­ä¼šåŒ…å« `tool_calls` å­—æ®µï¼Œè€Œé `content`ã€‚\n",
    "\n",
    "### ğŸ› ï¸ ä»£ç è°ƒç”¨ç¤ºä¾‹\n",
    "\n",
    "#### Python (requests)\n",
    "```python\n",
    "import requests, json\n",
    "url = \"http://localhost:8080/v1/chat/completions\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "data = {\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"ä½ å¥½\"}],\n",
    "    \"stream\": False\n",
    "}\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=2, ensure_ascii=False))\n",
    "```\n",
    "\n",
    "#### Python (OpenAI SDK)\n",
    "llama.cpp æœåŠ¡å™¨ä¸ OpenAI SDK å®Œå…¨å…¼å®¹ã€‚\n",
    "```python\n",
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"not-needed\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"ä½ å¥½\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "### âœ… å…³é”®æç¤ºä¸å»ºè®®\n",
    "1.  **æ¨¡å‹ä¸æ¨¡æ¿**ï¼šå¦‚éœ€ä½¿ç”¨å‡½æ•°è°ƒç”¨ç­‰é«˜çº§åŠŸèƒ½ï¼Œè¯·åŠ¡å¿…é€‰æ‹©**åŸç”Ÿæ”¯æŒè¯¥åŠŸèƒ½çš„æ¨¡å‹**ï¼ˆå¦‚ Llama 3.1ã€Qwen 2.5ç³»åˆ—ï¼‰ï¼Œå¹¶ä½¿ç”¨æ­£ç¡®çš„ `--chat-template` å‚æ•°å¯åŠ¨æœåŠ¡å™¨ã€‚\n",
    "2.  **å†…å®¹ç±»å‹**ï¼šæ‰€æœ‰è¯·æ±‚å¤´éœ€åŒ…å« `Content-Type: application/json`ã€‚\n",
    "3.  **å‚æ•°è°ƒæ•´**ï¼šæ ¸å¿ƒå‚æ•° `temperature` (éšæœºæ€§)ã€`max_tokens` (ç”Ÿæˆé•¿åº¦)ã€`top_p` ç­‰å¯æ ¹æ®éœ€æ±‚è°ƒæ•´ã€‚\n",
    "4.  **è§£å†³è¶…é•¿è¾“å…¥**ï¼šè¾“å…¥è¿‡é•¿å¯èƒ½å¯¼è‡´é”™è¯¯ï¼Œå¯æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—å¹¶æ ¹æ®æç¤ºè°ƒæ•´ `--ctx-size` å¯åŠ¨å‚æ•°ã€‚\n",
    "\n",
    "å¦‚æœä½ èƒ½å‘Šè¯‰æˆ‘ä½ å…·ä½“æƒ³ç”¨ llama.cpp çš„ HTTP æ¥å£æ¥å®Œæˆä»€ä¹ˆä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œç®€å•çš„é—®ç­”å¯¹è¯ã€é•¿æ–‡æ¡£å¤„ç†ï¼Œè¿˜æ˜¯æƒ³å®ç°ç±»ä¼¼AI Agentçš„å‡½æ•°è°ƒç”¨ï¼‰ï¼Œæˆ‘å¯ä»¥æä¾›æ›´å…·ä½“çš„å‚æ•°é…ç½®å»ºè®®æˆ–ç¤ºä¾‹ä»£ç ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
