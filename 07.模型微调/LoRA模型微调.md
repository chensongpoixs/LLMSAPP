# LoRA模型微调

![](/img/LoRA工作原理流程图.svg)

1. 称作: 低秩适配
2. 秩: 3X5的矩阵，其秩（r）为: ${r <= min(3, 5)}$
3. 为什么要使用LoRA

    - 全量微调太贵了
    - 微调7B模型， 需要100G+显存(参数+梯度+优化器状态)
    - LoRA:显存降低60%~80%，参数量仅有需更新0.1%~1%之间



## LoRA工作原理

1. LoRA用两个较小的矩阵表示一个较大的矩阵， 减少参数量


### LoRA更准确的认识

1. 需要<font color='red'>更新的参数</font>大量减少
2. 原因是用两个小矩阵表示原来的大矩阵
3. 而原来模型的参数依然要加载到显存中


数学公式

${y = W_0 * x + W *x = W_0 * x +a/r(BAx)}$


A矩阵用于降维， B矩阵用于升维

LoRA训练的是"增量", 但不是替换原参数

r是秩，a/r是缩放因子， 它可以平衡因子变化导致的值的变化


## 如何使用LoRA

1. LoRA作用于层，为该层的大矩阵增加一种两小矩阵的表达方式
2. huggingface社区为我们提供了相应的库peft
3. 通过peft的配置参数能为指定层增加LoRA
4. 结果： 原来的层+（LoRA_A@LoRA_B）
5. 初始时，将LoRA_A设置为高斯随便值，LoRA_B为0



# 7B模型能爆80G显存

1. 7B=700000000000 = 1400000 ~ 14G(FP16)
2. 训练是，每个参数都记录着其梯度值~ 14G(FP16)
3. 优化器

- AdamW为每个参数维护着两份信息:动量；方差(FP32)
- 除此之外,工程上还备份了一份权重 （FP32）


# 全量微调需要的显存

1. 7B=7000000000=14000000000~14G(FP16)
2. 反向传播中的梯度:7Bx2=14G
3. 动量:7Bx4=28G
4. 方差:7Bx4=28G
5. 备份参数:7Bx4=28G
6. 总共需要:112G


# LoRA需要的显存

1. 7B=70 0000000=14000000000 ≈14G(FP16)显存冻结所有参数，因此梯度和优化器都不占用
2. LORA参数:0.2G
3. LORA梯度:0.2G
4. LORA优化器:1.20
5. 总共需要:16G


# 总结




