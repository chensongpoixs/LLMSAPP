{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed68cde3",
   "metadata": {},
   "source": [
    "# 一、 加载Dataset数据集\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84bfb17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Edit the following sentence for grammar.', 'input': 'He go to the park every day.', 'output': 'He goes to the park every day.'}\n",
      "1100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"instruction-data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data[1])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07122ed",
   "metadata": {},
   "source": [
    "# 二、 送训练 request和response格式\n",
    "\n",
    "```javascript \n",
    "\n",
    "\"Below is an instruction that describes a task.\"\n",
    "\"Write a response that appropriately completes the request.\"\n",
    "\"\\n\\n### Instruction:\\n   instruction\"\n",
    "\n",
    "\"\\n\\n### Input:\\n input\" \n",
    "\n",
    " \n",
    "\"\\n\\n### Response:\\noutput\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff535966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "def format_input(item):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task.\"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{item['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{item['input']}\" if item[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text\n",
    "\n",
    "myinput = format_input(data[50])\n",
    "response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "print(myinput+response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a28f7e",
   "metadata": {},
   "source": [
    "# 三、 训练数据集、验证数据集、测试数据解的比列\n",
    "\n",
    "训练数据集:80%\n",
    "\n",
    "验证数据集:10%\n",
    "\n",
    "测试数据集:10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52569fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set length: 880\n",
      "val set length: 110\n",
      "test set length: 110\n"
     ]
    }
   ],
   "source": [
    "#8:1:1\n",
    "train_part = (int)(len(data) *0.8)\n",
    "val_part = (int)(len(data)*0.1)\n",
    "test_part = len(data)-train_part - val_part\n",
    "\n",
    "train_data = data[:train_part]\n",
    "val_data = data[train_part:train_part+val_part]\n",
    "test_data = data[train_part+val_part:]\n",
    "\n",
    "print(\"train set length:\", len(train_data))\n",
    "print(\"val set length:\", len(val_data))\n",
    "print(\"test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba477ee",
   "metadata": {},
   "source": [
    "# 四、构建Dataset的数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dea3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.samples = []\n",
    "        for i in data:\n",
    "            input = format_input(i)\n",
    "            response = f\"\\n\\n### Response:\\n{i['output']}\"\n",
    "            full_text = input + response\n",
    "            self.samples.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.samples[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ff7335e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c2f3e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch;\n",
    "\n",
    "def my_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_token_id=-100,\n",
    "    allowed_max_length=None, # 最大\n",
    "    device=\"cpu\"  # 默认cpu设备\n",
    "):\n",
    "    # 获取这个批次中最长样本的长度\n",
    "    batch_max_len = max(len(i) +1 for i in batch );\n",
    "\n",
    "    # 输入列表和目标列表\n",
    "    input_list, target_list = [], [];\n",
    "\n",
    "    for i in batch:\n",
    "        # 1. 将这个批次中小于批次最大长度的所有样本进行填充\n",
    "\n",
    "        # 2. 根据输入创建targets\n",
    "\n",
    "        # 3. 将targets中填充的token_id替换成-100(除了第一个的填充tokenid之外)\n",
    "        new_item = i + [pad_token_id];\n",
    "        padded = new_item +[pad_token_id] * (batch_max_len - len(new_item));\n",
    "\n",
    "        inputs = torch.tensor( padded[:-1]);\n",
    "        targets = torch.tensor(padded[1:]);\n",
    "\n",
    "        # targets  = [ 1 2 3   50256   50256 50256 ...]\n",
    "        # mask = [False, False, False True, True ...] \n",
    "        mask = targets == pad_token_id;\n",
    "\n",
    "        slice = torch.nonzero(mask).sqeeze();\n",
    "\n",
    "        # 不只一个大于1\n",
    "        if slice.numel() > 1:\n",
    "            targets[slice[1:]] = ignore_token_id;\n",
    "\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = input[:allowed_max_length];\n",
    "            targets = targets[:allowed_max_length];\n",
    "\n",
    "        input_list.append(inputs);\n",
    "        target_list.append(targets);\n",
    "\n",
    "    input_tensor = torch.stack(input_list).to(device);\n",
    "\n",
    "    target_tensor = torch.stack(target_list); #.to(device);\n",
    "\n",
    "    return input_tensor, target_tensor;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a159052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [3],\n",
      "        [4],\n",
      "        [6],\n",
      "        [7]])\n",
      "tensor([1, 3, 4, 6, 7])\n",
      "tensor([    1,    34,     4,  -100,  -100, 50256,  -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "test_mask = [False, True, False, True, True, False, True, True];\n",
    "# 打印非 False的下标\n",
    "print(torch.nonzero(torch.tensor(test_mask)));\n",
    "# 二维变一维\n",
    "slice = torch.nonzero(torch.tensor(test_mask)).squeeze();\n",
    "print(slice);\n",
    "\n",
    "\n",
    "targets = torch.tensor([1, 34, 4, 5, 6, 50256, 50256, 50256]);\n",
    "# 前面三项不动 改变后面的数字\n",
    "targets[slice[1:]] = -100;\n",
    "print(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7cd81ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from functools import partial\n",
    "customized_collate_fn = partial(my_collate_fn, \n",
    "                                device=device,\n",
    "                                allowed_max_length=1024)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c157eaf",
   "metadata": {},
   "source": [
    "# 五、加载 训练、验证数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "486f9716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  加载数据集\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = customized_collate_fn,\n",
    "    shuffle = True,\n",
    "    drop_last = True\n",
    ")\n",
    "\n",
    "#验证数据集\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = customized_collate_fn,\n",
    "    shuffle = False,\n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = customized_collate_fn,\n",
    "    shuffle = False,\n",
    "    drop_last = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4978fbc8",
   "metadata": {},
   "source": [
    "# 六、加载OpenAi-GPT2模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89112e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/3] 正在从 Hugging Face 下载/加载 gpt2 模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai-community/gpt2/resolve/main/config.json (Caused by ConnectTimeoutError(<HTTPSConnection(host='huggingface.co', port=443) at 0x23440041690>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 83d4a383-61a4-477e-bccb-614d21ccd46c)')' thrown while requesting HEAD https://huggingface.co/openai-community/gpt2/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai-community/gpt2/resolve/main/config.json (Caused by ConnectTimeoutError(<HTTPSConnection(host='huggingface.co', port=443) at 0x23440041900>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: fbfc112f-f3a9-4cd1-8f67-7e54cea96a95)')' thrown while requesting HEAD https://huggingface.co/openai-community/gpt2/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai-community/gpt2/resolve/main/config.json (Caused by ConnectTimeoutError(<HTTPSConnection(host='huggingface.co', port=443) at 0x23440041b70>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: e49bec8a-40f9-41fd-a4e4-516222f1a5e4)')' thrown while requesting HEAD https://huggingface.co/openai-community/gpt2/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai-community/gpt2/resolve/main/config.json (Caused by ConnectTimeoutError(<HTTPSConnection(host='huggingface.co', port=443) at 0x23440041fc0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 4bfe5a22-6645-4588-8b93-4397ece30793)')' thrown while requesting HEAD https://huggingface.co/openai-community/gpt2/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a228733c48b40f28416bec5be74ed8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7141123e7f80443aaa926938ed24deea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b55045e95a4a7ea63a7c81534c0c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/3] 开始权重移植...\n",
      "  -> 正在加载 Embeddings (wte, wpe)...\n",
      "  -> 正在加载 12 层 Transformer Block...\n",
      "  -> 正在加载 Final LayerNorm & Head...\n",
      "[3/3] 成功！GPT-2 权重已全部加载完成。\n",
      "\n",
      "模型已加载至：cuda\n"
     ]
    }
   ],
   "source": [
    "from GPTModel import MyGPTModel, generate_new, text_to_tokenids, tokenids_to_text\n",
    "from load_gpt2_model import load_gpt2_weights\n",
    "import torch\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"max_seq_length\": 1024,\n",
    "    \"embedding_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "model = MyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "load_gpt2_weights(model, GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"模型已加载至：{device}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
