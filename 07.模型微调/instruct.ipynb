{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed68cde3",
   "metadata": {},
   "source": [
    "# 一、 加载Dataset数据集\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bfb17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"instruction-data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data[1])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07122ed",
   "metadata": {},
   "source": [
    "# 二、 送训练 request和response格式\n",
    "\n",
    "```javascript \n",
    "\n",
    "\"Below is an instruction that describes a task.\"\n",
    "\"Write a response that appropriately completes the request.\"\n",
    "\"\\n\\n### Instruction:\\n   instruction\"\n",
    "\n",
    "\"\\n\\n### Input:\\n input\" \n",
    "\n",
    " \n",
    "\"\\n\\n### Response:\\noutput\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff535966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(item):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task.\"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{item['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{item['input']}\" if item[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text\n",
    "\n",
    "myinput = format_input(data[50])\n",
    "response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "print(myinput+response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a28f7e",
   "metadata": {},
   "source": [
    "# 三、 训练数据集、验证数据集、测试数据解的比列\n",
    "\n",
    "训练数据集:80%\n",
    "\n",
    "验证数据集:10%\n",
    "\n",
    "测试数据集:10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52569fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8:1:1\n",
    "train_part = (int)(len(data) *0.8)\n",
    "val_part = (int)(len(data)*0.1)\n",
    "test_part = len(data)-train_part - val_part\n",
    "\n",
    "train_data = data[:train_part]\n",
    "val_data = data[train_part:train_part+val_part]\n",
    "test_data = data[train_part+val_part:]\n",
    "\n",
    "print(\"train set length:\", len(train_data))\n",
    "print(\"val set length:\", len(val_data))\n",
    "print(\"test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba477ee",
   "metadata": {},
   "source": [
    "# 四、构建Dataset的数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dea3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.samples = []\n",
    "        for i in data:\n",
    "            input = format_input(i)\n",
    "            response = f\"\\n\\n### Response:\\n{i['output']}\"\n",
    "            full_text = input + response\n",
    "            self.samples.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.samples[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff7335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f3e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch;\n",
    "\n",
    "# def my_collate_fn(\n",
    "#     batch,\n",
    "#     pad_token_id=50256,\n",
    "#     ignore_token_id=-100,\n",
    "#     allowed_max_length=None, # 最大\n",
    "#     device=\"cuda\"  # 默认cpu设备\n",
    "# ):\n",
    "#     # 获取这个批次中最长样本的长度\n",
    "#     batch_max_len = max(len(i) +1 for i in batch );\n",
    "\n",
    "#     # 输入列表和目标列表\n",
    "#     input_list, target_list = [], [];\n",
    "\n",
    "#     for i in batch:\n",
    "#         # 1. 将这个批次中小于批次最大长度的所有样本进行填充\n",
    "\n",
    "#         # 2. 根据输入创建targets\n",
    "\n",
    "#         # 3. 将targets中填充的token_id替换成-100(除了第一个的填充tokenid之外)\n",
    "#         new_item = i + [pad_token_id];\n",
    "#         padded = new_item +[pad_token_id] * (batch_max_len - len(new_item));\n",
    "\n",
    "#         inputs = torch.tensor( padded[:-1]);\n",
    "#         targets = torch.tensor(padded[1:]);\n",
    "\n",
    "#         # targets  = [ 1 2 3   50256   50256 50256 ...]\n",
    "#         # mask = [False, False, False True, True ...] \n",
    "#         mask = targets == pad_token_id;\n",
    "\n",
    "#         slice = torch.nonzero(mask).sqeeze();\n",
    "#         #slice = torch.nonzero(mask);\n",
    "#         # 不只一个大于1\n",
    "#         if slice.numel() > 1:\n",
    "#             targets[slice[1:]] = ignore_token_id;\n",
    "\n",
    "#         if allowed_max_length is not None:\n",
    "#             inputs = inputs[:allowed_max_length];\n",
    "#             targets = targets[:allowed_max_length];\n",
    "\n",
    "#         input_list.append(inputs);\n",
    "#         target_list.append(targets);\n",
    "\n",
    "#     input_tensor = torch.stack(input_list).to(device);\n",
    "\n",
    "#     target_tensor = torch.stack(target_list); # .to(device);\n",
    "\n",
    "#     return input_tensor, target_tensor;\n",
    "\n",
    "\n",
    "def my_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_token_id=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    #获取这个批次中最长样本的长度\n",
    "    batch_max_len = max(len(i)+1 for i in batch)\n",
    "    #bacth_max_len = max(len(i)+1 for i in batch)\n",
    "    input_list, target_list = [], []\n",
    "\n",
    "    for i in batch:\n",
    "        #将这个批次中小于批次最大长度的所有样本进行填充\n",
    "        #根据输入创建targets\n",
    "        #将targets中填充的token_id替换成-100（除第一个填充的tokenid之外）\n",
    "        new_item = i + [pad_token_id]\n",
    "        padded = new_item + [pad_token_id] * (batch_max_len - len(new_item))\n",
    "        #padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        # targets = [1 2 3 50256 50256 ...],\n",
    "        # mask = [False, False, False, True, True ...]\n",
    "        mask = targets == pad_token_id\n",
    "        slice = torch.nonzero(mask).squeeze()\n",
    "        if slice.numel() > 1:\n",
    "            targets[slice[1:]] = ignore_token_id\n",
    "        \n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "        \n",
    "        input_list.append(inputs)\n",
    "        target_list.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(input_list).to(device)\n",
    "    #targets_tensor = troch.stack(target_list)\n",
    "    targets_tensor = torch.stack(target_list)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a159052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_mask = [False, True, False, True, True, False, True, True];\n",
    "# # 打印非 False的下标\n",
    "# print(torch.nonzero(torch.tensor(test_mask)));\n",
    "# # 二维变一维\n",
    "# slice = torch.nonzero(torch.tensor(test_mask)).squeeze();\n",
    "# print(slice);\n",
    "\n",
    "\n",
    "# targets = torch.tensor([1, 34, 4, 5, 6, 50256, 50256, 50256]);\n",
    "# # 前面三项不动 改变后面的数字\n",
    "# targets[slice[1:]] = -100;\n",
    "# print(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd81ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from functools import partial\n",
    "customized_collate_fn = partial(my_collate_fn, \n",
    "                                device=device,\n",
    "                                allowed_max_length=1024)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c157eaf",
   "metadata": {},
   "source": [
    "# 五、加载 训练、验证数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f9716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  加载数据集\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = customized_collate_fn,\n",
    "    shuffle = True,\n",
    "    drop_last = True\n",
    ")\n",
    "\n",
    "#验证数据集\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = customized_collate_fn,\n",
    "    shuffle = False,\n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = customized_collate_fn,\n",
    "    shuffle = False,\n",
    "    drop_last = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4978fbc8",
   "metadata": {},
   "source": [
    "# 六、加载OpenAi-GPT2模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89112e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai-community/gpt2/resolve/main/config.json (Caused by ConnectTimeoutError(<HTTPSConnection(host='huggingface.co', port=443) at 0x1619ca66050>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 1e42ddc3-ea9d-48bd-b864-ec0f603f6a80)')' thrown while requesting HEAD https://huggingface.co/openai-community/gpt2/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai-community/gpt2/resolve/main/config.json (Caused by ConnectTimeoutError(<HTTPSConnection(host='huggingface.co', port=443) at 0x1619ca67460>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 08aaf6ba-5122-444c-9b3d-8b592549b0a0)')' thrown while requesting HEAD https://huggingface.co/openai-community/gpt2/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai-community/gpt2/resolve/main/config.json (Caused by ConnectTimeoutError(<HTTPSConnection(host='huggingface.co', port=443) at 0x1619ca66470>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 022da1cf-e2c7-4d2d-a3a0-5669a883141f)')' thrown while requesting HEAD https://huggingface.co/openai-community/gpt2/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/3] 开始权重移植...\n",
      "  -> 正在加载 Embeddings (wte, wpe)...\n",
      "  -> 正在加载 12 层 Transformer Block...\n",
      "  -> 正在加载 Final LayerNorm & Head...\n",
      "[3/3] 成功！GPT-2 权重已全部加载完成。\n",
      "\n",
      "模型已加载至：cuda\n"
     ]
    }
   ],
   "source": [
    "from GPTModel import MyGPTModel, generate_new, text_to_tokenids, tokenids_to_text\n",
    "from load_gpt2_model import load_gpt2_weights\n",
    "import torch\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"max_seq_length\": 1024,\n",
    "    \"embedding_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": True\n",
    "};\n",
    "model = MyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "load_gpt2_weights(model, GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"模型已加载至：{device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0442c2a",
   "metadata": {},
   "source": [
    "#  八、 加载模型 推理demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa5aed3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:OpenAI is the first game to be built specifically for the platform. It's based on a popular AI software, and includes tools for building games to be played on\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "prompt = \"OpenAI is\"\n",
    "prompt = text_to_tokenids(prompt, tokenizer).to(device)\n",
    "tokens = generate_new(model, \n",
    "             prompt,\n",
    "             30,\n",
    "             GPT_CONFIG_124M[\"max_seq_length\"],\n",
    "             25,\n",
    "             1.2)\n",
    "\n",
    "print(f\"output:{tokenids_to_text(tokens, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e434076",
   "metadata": {},
   "source": [
    "# 九、 指令微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0403d9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: tensor(4., device='cuda:0')\n",
      "Validation loss: tensor(4., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from GPTModel import calc_loss\n",
    "with torch.no_grad():\n",
    "        train_loss = calc_loss(train_dataloader, model, device)\n",
    "        val_loss = calc_loss(val_dataloader, model, device)\n",
    "\n",
    "\n",
    "print(\"Training loss:\", train_loss);\n",
    "print(\"Validation loss:\", val_loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269db69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1 step:         5: Train loss 1.000, Eval loss1.000\n",
      "Epoch1 step:        10: Train loss 1.000, Eval loss1.000\n",
      "Epoch1 step:        15: Train loss 1.000, Eval loss1.000\n",
      "Epoch1 step:        20: Train loss 1.000, Eval loss1.000\n",
      "Epoch1 step:        25: Train loss 1.000, Eval loss1.000\n"
     ]
    }
   ],
   "source": [
    "from GPTModel import train_model;\n",
    "import time;\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time();\n",
    "\n",
    "torch.manual_seed(123);\n",
    "\n",
    "# 优化器设置\n",
    "optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.001);\n",
    "\n",
    "\n",
    "epochs = 2;\n",
    "\n",
    "train_loss, val_loss = train_model(model, \n",
    "           train_dataloader,\n",
    "           val_dataloader,\n",
    "            optimizer=optimizer, \n",
    "            device=device,\n",
    "            epochs=epochs,\n",
    "            tokenizer=tokenizer,\n",
    "            eval_interval=5,\n",
    "            \n",
    "            prompt=format_input(val_data[0]),)\n",
    "\n",
    "\n",
    "end_time = time.time();\n",
    "\n",
    "duration = (end_time-start_time)/60;\n",
    "print(f\"Traning Finished, in{duration:.2f} minutes.\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129c3584",
   "metadata": {},
   "source": [
    "#  保存模型到文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bc992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_sft.pth\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d8ed79",
   "metadata": {},
   "source": [
    "# 绘制损失趋势图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13776c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as  plt;\n",
    "\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    \"\"\"Safely convert any tensor to numpy array\"\"\"\n",
    "    return tensor.detach().cpu().numpy()\n",
    "\n",
    "def plot_loss(epochs, train_loss, val_loss ):\n",
    "    fig, ax = plt.subplots(figsize=(5, 3));\n",
    "    ax.plot(epochs, train_loss  , label=\"Traning loss\");\n",
    "    ax.plot(epochs, val_loss , label=\"Validation loss\");\n",
    "    ax.set_xlabel(\"epochs\");\n",
    "    ax.set_ylabel(\"Loss\");\n",
    "    ax.legend(loc=\"upper right\");\n",
    "    fig.tight_layout();\n",
    "\n",
    "x = torch.linspace(0, epochs, len(train_loss) );\n",
    "# 方法2：列表推导式（最常用）\n",
    "train_loss_cpu_list = [tensor.cpu() for tensor in train_loss];\n",
    "val_loss_cpu_list = [tensor.cpu() for tensor in val_loss];\n",
    "plot_loss(x,  (train_loss_cpu_list),  (val_loss_cpu_list));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e175d79",
   "metadata": {},
   "source": [
    "# 十、使用自己训练模型推理\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c63f487",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "for entry in test_data[:3]:                #A\n",
    "    input_text = format_input(entry)\n",
    "    token_ids = generate_new(model,\n",
    "        text_to_tokenids(input_text, tokenizer).to(device),\n",
    "        256,\n",
    "        GPT_CONFIG_124M[\"max_seq_length\"],\n",
    "        25,\n",
    "        1.0,\n",
    "        50256)\n",
    "    generated_text = tokenids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\",\"\").strip()\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
