# 深度神经网络之Dropout

1. L2正则化虽然可以有效降低过拟合，但也有局限性
2. 当神经网络特别复杂时，仍然会出现过拟合
3. 一种新的正则化提了出来， 就是Dropout

## Dropout如何工作

1. 在神经网络的每一层随机让一下神经元失效

### 经典 Dropout 示例：训练 vs 推理

![经典 Dropout 示例](./dropout_flow.svg)

**图示说明：**
- **左侧面板（训练阶段）**：展示在深度神经网络的两层隐藏层中应用Dropout的过程。以概率 p=0.5 随机关闭部分神经元（用红色圆圈表示），这些被关闭的神经元不参与前向和反向传播。
- **右侧面板（推理阶段）**：推理时所有神经元都参与计算，但权重会按照 (1-p) 进行缩放（Inverted Dropout），以保证期望输出的一致性。

### 工作原理

#### 训练阶段（Training with Dropout）

1. **随机掩码（Stochastic Masking）**：
   - 对于每个小批次（mini-batch），以概率 $p$ 随机选择要丢弃的神经元
   - 丢弃的神经元的激活值被设置为零

2. **Inverted Dropout 缩放**：
   - 被保留的神经元的输出除以 $(1-p)$ 进行缩放
   - 公式：$\tilde{a} = \frac{a \cdot m}{1-p}$，其中 $m \sim \text{Bernoulli}(1-p)$

3. **反向传播**：
   - 梯度只通过活跃（保留）的神经元流动
   - 被丢弃的神经元不更新权重参数

#### 推理阶段（Inference without Dropout）

1. **关闭Dropout**：在测试和推理时，所有神经元都参与计算，不应用随机丢弃
2. **权重缩放**：
   - 如果训练时使用了 Inverted Dropout，推理时可以直接使用原始权重（已缩放）
   - 或者在训练时不缩放，推理时权重乘以 $(1-p)$

### Dropout 的优势与特性

**优势：**
- 强制网络学习冗余表示，使网络对输入变化更鲁棒
- 减少特征共适应（co-adaptation），提高特征独立性
- 有效降低过拟合，改善泛化性能
- 实现简单，计算开销小

**特性：**
- 丢弃率 $p$ 通常在 0.5 左右
- 输入层通常使用较小的 $p$（如 0.1-0.2）
- 隐藏层可以使用较大的 $p$（如 0.5）
- 不影响网络的表达能力，只改变训练动态
