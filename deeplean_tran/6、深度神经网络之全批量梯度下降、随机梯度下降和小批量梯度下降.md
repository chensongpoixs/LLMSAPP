# 深度神经网络之全批量梯度下降、随机梯度下降和小批量梯度下降（mini-batch size）

使用整个数据集计算梯度， 之后在更新模型参数



## 随机梯度下降与小批量梯度下降


### 1、随机梯度下降

<font color='red'>随机梯度下降也称SGD，每次使用一个样本进行梯度更新</font>

<h1><font color='red'>随机梯度降的优缺点</font></h1>

1. 每次使用一个样本更新一次参数，计算量小
2. 它可以更快地进行参数调整，有可能加速训练过程
3. 无法充分利用GPU硬件
4. 收敛不稳定，容易出现较大的抖动

### 2、 小批量梯度下降

<font color='red'>将数据集分成多个小批次， 每个批次都更新一个模型参数</font>


<h1><font color='red'>小批量的优点</font></h1>

- 学习的更快:梯度下降的速度比全批量梯度下降的速度快
- 收敛稳定:它比随机梯度下降收敛的更稳定
- 利用GPU硬件更充分

<h1><font color='red'>小批量的缺点</font></h1>

- 引入了新的超参数： mini-batch size
- 如果超参数设置不好， 会引起梯度下降的剧烈震荡

