#  深度学习之向量化与矩阵化

## 一、线性回归向量化的实现


公式: 

${z = W^T X  + b}$

${z = w_1*x_1 +w_2*x_2 + ... + b(神经元)}$


### 1、非向量化代码实现

```javascript

for i in rang(n):
    z += w[i] * x[j];

z += b;

```

### 2、向量化的代码实现

```javascript

w_t = w.T;
z = np.dot(w_t, x) + b;

```


## 二、逻辑回归的向量化实现


![逻辑回归的向量化实现](/deeplean_tran/mermaid_flow.svg)

### 1. 结构图（Mermaid）

```mermaid
graph LR
    subgraph Neuron[单个神经元（向量化）]
        X[输入向量 X<br/>(形状: n × m)] --> DOT[线性组合 z = W^T X + b]
        W[权重 W<br/>(形状: n × 1)] --> DOT
        b[偏置 b<br/>(标量或 1 × m 广播)] --> DOT
        DOT --> SIG[激活 a = sigmoid(z)]
        SIG --> Yhat[预测 ŷ (1 × m)]
    end
```

> 说明：通常 `X` 的形状为 `(n, m)`（n 为特征数，m 为样本数），`W` 为 `(n, 1)`，因此 `z = W^T X + b` 的结果为 `(1, m)`。

### 2. 流程图（Mermaid）

```mermaid
flowchart TD
    A[输入: X (n,m), Y (1,m), W (n,1), b] --> B[前向：z = W^T X + b  (1,m)]
    B --> C[激活：A = sigmoid(z)  (1,m)]
    C --> D[计算损失：L = -1/m * sum(Y*log(A) + (1-Y)*log(1-A))]
    D --> E[反向：dZ = A - Y  (1,m)]
    E --> F[梯度：dW = 1/m * X · dZ^T  (n,1) \n db = 1/m * sum(dZ)]
    F --> G[参数更新：W := W - lr * dW; b := b - lr * db]
    G --> H[重复直到收敛]
```

### 3. 关键向量化公式（简洁版）

- 前向

    - z = W^T X + b               (结果形状: 1 × m)
    - A = sigmoid(z)              (1 × m)

- 损失（对 m 个样本取平均）

    - J = -1/m * sum( Y * log(A) + (1 - Y) * log(1 - A) )

- 反向（梯度）

    - dZ = A - Y                  (1 × m)
    - dW = 1/m * X · dZ^T         (n × 1)
    - db = 1/m * sum(dZ)          (标量)

### 4. 向量化 NumPy 代码示例

```python
import numpy as np

def sigmoid(z):
        return 1 / (1 + np.exp(-z))

# X shape: (n, m), Y shape: (1, m)
# W shape: (n, 1), b is scalar (or shape (1,))
Z = np.dot(W.T, X) + b      # (1, m)
A = sigmoid(Z)              # (1, m)
m = X.shape[1]

# cost
cost = -np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) / m

# gradients
dZ = A - Y                  # (1, m)
dW = np.dot(X, dZ.T) / m    # (n, 1)
db = np.sum(dZ) / m         # scalar

# update
W = W - learning_rate * dW
b = b - learning_rate * db
```

### 5. 使用提示

- 对于大规模数据，使用小批量（mini-batch）来做向量化更新；保证 `X` 的列为样本，行为特征。
- 注意数值稳定性：计算 `log(1-A)` 时可用 `np.clip(A, 1e-12, 1-1e-12)` 防止 NaN。
- 若需要将这些 Mermaid 图导出为图片（PNG/SVG），可以使用 VS Code 的 Mermaid Preview 或在线渲染器。


## 三、矩阵化之多神经元



 