# 深度学习中激活函数

![](/deeplearn/activation_functions_overview.svg)

1. Sigmoid
2. Tanh (Sigmoid改进版)
3. ReLU（Rectified Lieaner Unit）
4. Leaky ReLU
5. Softmax (分类任务中)



![](/deeplearn/activation_curves.svg)
## 简要说明

- **选择原则**：在隐藏层优先使用 ReLU 系列（ReLU、LeakyReLU、ELU、SELU）；Transformer/大型模型推荐 GELU/Swish；输出层根据任务选择 Sigmoid（二分类）或 Softmax（多分类）。
- **常见问题与解决**：若出现训练不稳定或梯度消失，尝试更换激活、使用批标准化、改进权重初始化或用自适应优化器（Adam）。

更多细节见图中每种激活函数的公式、性质与应用场景。

## 示例代码与运行

仓库中包含演示脚本 `deeplearn/activation_examples.py`，用 NumPy 实现了常见激活函数并对前向计算做了简单测速。运行（在 `deeplearn` 目录下）：

```powershell
pip install numpy
python activation_examples.py
```

脚本输出每个激活函数在随机输入上的均值、标准差与平均耗时，并在末尾给出简短说明。可作为理解激活函数行为的快速参考。