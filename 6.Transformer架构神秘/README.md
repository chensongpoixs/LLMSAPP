# Transformer架构


[Google 2017年分布 Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)


我来为你详细绘制Transformer的编码器和解码器流程图。

## **Transformer 整体架构图**

```
输入序列 → 编码器 → 解码器 → 输出序列
```

## **1. 编码器（Encoder）流程图**

```
┌─────────────────────────────────────────────────────────┐
│                    编码器 (Encoder)                       │
└─────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────┐
│               输入序列 (Input Sequence)                  │
└─────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────┐
│         嵌入层 + 位置编码                               │
│  (Embedding + Positional Encoding)                     │
└─────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────┐
│                N × 编码器层                             │
│                (N × Encoder Layer)                      │
├─────────────────────────────────────────────────────────┤
│  层 1:                                                  │
│  ┌────────────────────┐    ┌────────────────────┐     │
│  │ 多头自注意力       │ →  │ 前馈神经网络       │     │
│  │ (Multi-Head        │    │ (Feed Forward)     │     │
│  │  Self-Attention)   │    └────────────────────┘     │
│  └────────────────────┘              ↑                │
│           ↑                          │ 残差连接        │
│           │ 残差连接                 │ + 层归一化      │
│           │ + 层归一化               │                │
└─────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────┐
│                编码器输出                              │
│            (Encoder Output)                            │
│  包含输入序列的上下文表示                               │
└─────────────────────────────────────────────────────────┘
```

**编码器详细步骤：**
1. **输入处理**：分词 → 词嵌入 → 位置编码
2. **多头自注意力机制**：
   ```
   输入 → 线性变换 → 分成多个头 → 计算注意力 → 拼接 → 线性变换
   ```
3. **前馈神经网络**：两层线性变换 + ReLU激活
4. **残差连接 + 层归一化**：每个子层后都有
5. **重复N次**（原始论文中N=6）

## **2. 解码器（Decoder）流程图**

```
┌─────────────────────────────────────────────────────────┐
│                    解码器 (Decoder)                       │
└─────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────┐
│               目标序列 (Target Sequence)                │
│               （右移一位，用于训练）                     │
└─────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────┐
│         嵌入层 + 位置编码                               │
│  (Embedding + Positional Encoding)                     │
└─────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────┐
│                N × 解码器层                             │
│                (N × Decoder Layer)                      │
├─────────────────────────────────────────────────────────┤
│  层 1:                                                  │
│  ┌────────────────────┐    ┌────────────────────┐     │
│  │ 掩码多头自注意力   │ →  │ 编码器-解码器      │ →   │
│  │ (Masked Multi-Head │    │ 注意力            │     │
│  │  Self-Attention)   │    │ (Encoder-Decoder  │     │
│  └────────────────────┘    │  Attention)       │     │
│           ↑                └────────────────────┘     │
│           │ 残差连接                 ↑                │
│           │ + 层归一化               │ 残差连接        │
│           │                         │ + 层归一化      │
├─────────────────────────────────────────────────────────┤
│                              ↓                         │
│              ┌────────────────────┐                   │
│              │ 前馈神经网络       │                   │
│              │ (Feed Forward)     │                   │
│              └────────────────────┘                   │
│                        ↑                              │
│                        │ 残差连接                     │
│                        │ + 层归一化                   │
└─────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────┐
│                 线性层 + Softmax                        │
│                (Linear + Softmax)                       │
└─────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────┐
│                输出概率分布                             │
│            (Output Probability)                         │
└─────────────────────────────────────────────────────────┘
```

**解码器详细步骤：**
1. **输入处理**：目标序列嵌入 + 位置编码
2. **掩码多头自注意力**：
   - 防止位置i关注位置j（j>i）
   - 确保自回归性质
3. **编码器-解码器注意力**：
   - Query来自解码器
   - Key和Value来自编码器输出
   - 连接两个序列的信息
4. **前馈神经网络**：与编码器相同
5. **线性层 + Softmax**：生成下一个词的概率分布

## **3. 编码器-解码器连接**

```
┌─────────────┐    编码器输出      ┌─────────────┐
│   编码器     │─────────────────→│   解码器     │
│             │   作为K和V        │             │
└─────────────┘                   └─────────────┘
                                      ↓
                               ┌─────────────┐
                               │   输出序列   │
                               └─────────────┘
```

## **4. 核心组件详解**

**多头注意力机制流程：**
```
输入 → 线性投影 → 分割成h个头 → 每个头：
    Q, K, V = 线性变换
    注意力分数 = softmax(Q·Kᵀ/√dₖ) · V
→ 拼接所有头 → 线性投影 → 输出
```

**训练 vs 推理流程对比：**

```
训练（Teacher Forcing）：
输入序列 → 编码器 → 解码器（并行处理整个目标序列）
                     ↓
               计算损失 ← 与真实标签比较

推理（自回归）：
输入序列 → 编码器 → 解码器（逐个生成token）
                     ↓
             生成token₁ → 加入输入 → 生成token₂ → ...
```

## **关键特点总结**

1. **编码器**：双向上下文，可以看到整个输入序列
2. **解码器**：
   - 自回归生成
   - 掩码确保只能看到之前的位置
   - 编码器-解码器注意力连接两个序列
3. **位置编码**：加入序列顺序信息
4. **残差连接**：缓解梯度消失
5. **层归一化**：稳定训练

这个架构使得Transformer能够处理序列到序列的任务，如机器翻译、文本摘要等，同时具有高度并行化的优势。


1. 位置编码
2. 多头注意力机制
3. 残差网络
4. 层归一化
5. 前馈神经网络


## 一、Transformer的输入

1. Embedding（向量编码）
2. Positional Encoding（位置编码）