{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5fb2687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨è¯»å– math_problems_100.json ...\n",
      "all_data:{'questions': [{'grade': 'å…­å¹´çº§', 'content': 'å·²çŸ¥ä¸€ä¸ªä¸‰è§’å½¢çš„åº•è¾¹é•¿ä¸º10å˜ç±³ï¼Œåº•è¾¹è§’ä¸º60åº¦ã€‚æ±‚ä¸‰è§’å½¢çš„é¢ç§¯ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œé•¿æ˜¯ 8 å˜ç±³ï¼Œå®½æ˜¯ 4 å˜ç±³ï¼Œæ±‚é•¿æ–¹å½¢çš„é¢ç§¯ï¼Œå¹¶è®¡ç®—é•¿æ–¹å½¢çš„å‘¨é•¿ã€‚è®¡ç®—æ–¹æ³•æ˜¯ï¼šé¢ç§¯ = é•¿ Ã— å®½ï¼Œå‘¨é•¿ = 2 Ã— (é•¿ + å®½)ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'å·²çŸ¥ä¸€ä¸ªæ­£æ–¹å½¢çš„è¾¹é•¿ä¸º10å˜ç±³ï¼Œé¢ç§¯æ˜¯20å¹³æ–¹å˜ç±³ã€‚å¦‚æœè¿™ä¸ªæ­£æ–¹å½¢çš„è¾¹é•¿å¢åŠ åˆ°12å˜ç±³ï¼Œåˆ™æ–°çš„é¢ç§¯æ˜¯å¤šå°‘å¹³æ–¹å˜ç±³ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§æ—¶ï¼Œæˆç»©æ˜¯85åˆ†ï¼Œå¹¶ä¸”åœ¨ä¸‰å¹´çº§æ—¶æ˜¯70åˆ†ï¼Œé‚£ä¹ˆå…­å¹´çº§å­¦ç”Ÿåœ¨ä¸‰å¹´çº§æ—¶æˆç»©çš„å¹³å‡åˆ†æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§æ•°å­¦å­¦ä¹ ä¸€å¹´ï¼Œä¸€å…±å­¦ä¹ äº† 20 ä¸ªè¯¾æ—¶ã€‚å¦‚æœå­¦ç”Ÿå¹³å‡æ¯å¹´å­¦ä¹  8 ä¸ªè¯¾æ—¶ï¼Œé‚£ä¹ˆå­¦ç”Ÿä¸€å¹´å­¦ä¹ å¤šå°‘è¯¾æ—¶ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼š 6å¹´çº§å­¦ç”Ÿå®Œæˆ 10 æ¬¡åŠ æ³•é¢˜ï¼Œæ€»å’Œä¸º 60ã€‚  è¯·å†™å‡ºç­”æ¡ˆã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'åœ¨ä¸‹åˆ—é¢˜ç›®ä¸­ï¼Œé€‰æ‹©æ­£ç¡®çš„è¿ç®—æ–¹æ³•ï¼Œå¹¶å†™å‡ºè®¡ç®—ç»“æœã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œé•¿æ˜¯ 8 å˜ç±³ï¼Œå®½æ˜¯ 4 å˜ç±³ã€‚æ±‚é•¿æ–¹å½¢çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œå®ƒçš„å‘¨é•¿æ˜¯ 24 å˜ç±³ã€‚æ±‚é•¿æ–¹å½¢çš„å‘¨é•¿æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'å·²çŸ¥ä¸€ä¸ªä¸‰è§’å½¢çš„åº•è¾¹é•¿ä¸º10å˜ç±³ï¼Œåº•è¾¹æ˜¯ä¸‰è§’å½¢çš„æ­£æ–¹å½¢éƒ¨åˆ†ï¼Œé«˜æ˜¯5å˜ç±³ã€‚æ±‚è¿™ä¸ªä¸‰è§’å½¢çš„é¢ç§¯ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'è¯·ç®€è¦æè¿°å…­å¹´çº§å­¦ç”Ÿçš„æ•°å­¦æ°´å¹³ï¼Œä¾‹å¦‚ï¼šåŸºç¡€ã€ä¸­çº§ã€é«˜çº§ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'åœ¨ä¸‹é¢çš„é¢˜ç›®ä¸­ï¼Œä½ éœ€è¦è®¡ç®—å‡ºå…­å¹´çº§çš„å­¦ç”Ÿéœ€è¦å®Œæˆçš„è®¡ç®—é‡ï¼Œå¹¶ä¸”è¦è§£é‡Šä½ çš„è®¡ç®—è¿‡ç¨‹ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªå•†ä½“çš„é”€å”®é¢æ˜¯1200å…ƒï¼Œæ¯›åˆ©ç‡ä¸º30%ã€‚å¦‚æœå•†ä½“å°†é”€å”®é¢å¢åŠ åˆ°1800å…ƒï¼Œæ¯›åˆ©ç‡å°†å¢åŠ å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'å·²çŸ¥ä¸€ä¸ªä¸‰è§’å½¢çš„åº•è¾¹é•¿ä¸º10å˜ç±³ï¼Œåº•è¾¹æ˜¯ç›´è§’ï¼Œåˆ™è¿™ä¸ªä¸‰è§’å½¢çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªæ­£æ–¹å½¢çš„è¾¹é•¿æ˜¯ 8 å˜ç±³ï¼Œå®ƒçš„é¢ç§¯æ˜¯å¤šå°‘å¹³æ–¹å˜ç±³ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'ä¸€ä¸ªå•†é‡å•†çš„åº“å­˜ï¼Œä»–æœ‰ 24 ä¸ªè‹¹æœï¼Œæœ‰ 12 ä¸ªæ¢¨ã€‚å¦‚æœä»–æƒ³æŠŠæ¢¨çš„æ•°é‡å¢åŠ  4 ä¸ªï¼Œä»–çš„æ€»åº“å­˜ä¼šå¢åŠ å¤šå°‘ä¸ªè‹¹æœï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªç§¯æœ¨çš„ä½“ç§¯æ˜¯ 24 ç«‹æ–¹å˜ç±³ï¼Œå¦‚æœç§¯æœ¨çš„è¾¹é•¿æ˜¯ 3 å˜ç±³ï¼Œé‚£ä¹ˆç§¯æœ¨çš„æ€»ä½“ç§¯æ˜¯å¤šå°‘ç«‹æ–¹å˜ç±³ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªä¸‰è§’å½¢çš„åº•è§’æ˜¯ 30 åº¦ï¼Œå¹¶ä¸”å®ƒçš„ä¸¤è¾¹åˆ†åˆ«æ˜¯ 10 å’Œ 15ï¼Œæ±‚ä¸‰è§’å½¢çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªç§¯æœ¨çš„ä½“ç§¯ä¸º 24 å¹³æ–¹å˜ç±³ï¼Œä¸€ä¸ªåœ†çš„åŠå¾„ä¸º 3 å˜ç±³ã€‚æ±‚ï¼šç§¯æœ¨å’Œåœ†çš„é¢ç§¯ã€‚è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªç§¯æœ¨çš„ä½“ç§¯æ˜¯ 12 å¹³æ–¹å˜ç±³ï¼Œåœ†çš„åŠå¾„æ˜¯ 4 å˜ç±³ï¼Œæ±‚ï¼šç§¯æœ¨å’Œåœ†çš„é¢ç§¯çš„ä¹˜ç§¯ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'å·²çŸ¥ï¼šä¸€ä¸ªè‹¹æœçš„ä»·æ ¼æ˜¯ 8 å…ƒï¼Œä¸€ä¸ªæ¢¨çš„ä»·æ ¼æ˜¯ 6 å…ƒã€‚å¦‚æœä¹°ä¸¤ä¸ªè‹¹æœå’Œä¸¤ä¸ªæ¢¨ï¼Œæ€»ä»·æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå·²çŸ¥ä¸€ä¸ªç§¯æœ¨çš„ä½“ç§¯æ˜¯ 24 å¹³æ–¹å˜ç±³ï¼Œå¦‚æœç§¯æœ¨çš„è¾¹é•¿æ˜¯ 3 å˜ç±³ï¼Œé‚£ä¹ˆç§¯æœ¨çš„æ€»ç§¯æœ¨æ•°æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªç§¯æœ¨çš„ä½“ç§¯æ˜¯ 24 ç«‹æ–¹å˜ç±³ï¼Œä¸€ä¸ªåœ†çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ã€‚å¦‚æœä¸¤ä¸ªç§¯æœ¨çš„ä½“ç§¯æ˜¯ 12 ç«‹æ–¹å˜ç±³ï¼Œé‚£ä¹ˆä¸¤ä¸ªåœ†çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿâ€ é™„å¸¦ä¸€äº›çº¿ç´¢ï¼Œä¾‹å¦‚â€œè®¡ç®—ä¸¤ä¸ªåœ†çš„é¢ç§¯â€å’Œâ€œæ ¹æ®ä½“ç§¯è®¡ç®—é¢ç§¯â€ç­‰ã€‚ é¢˜ç›®è¦æ±‚è®¡ç®—ç»“æœï¼Œå¹¶ç»™å‡ºç­”æ¡ˆã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œå®ƒçš„å‘¨é•¿æ˜¯ 24 å˜ç±³ã€‚æ±‚é•¿æ–¹å½¢çš„å‘¨é•¿æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œé•¿æ˜¯ 8 å˜ç±³ï¼Œå®½æ˜¯ 4 å˜ç±³ã€‚æ±‚é•¿æ–¹å½¢çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§æ•°å­¦å­¦ä¹ äº† 20 ä¸ªç»ƒä¹ é¢˜ï¼Œæ¯é¢˜éš¾åº¦ 5 åˆ†ã€‚å­¦ç”Ÿå¹³å‡åˆ†æ˜¯ 40 åˆ†ã€‚è®¡ç®—ï¼šå­¦ç”Ÿæ€»å…±éœ€è¦å¤šå°‘åˆ†æ‰èƒ½è¾¾åˆ° 40 åˆ†ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'ä¸€ä¸ªè‹¹æœé•¿äº†ä¸€ä¸ªæ¢¨ï¼Œè‹¹æœé•¿äº†3ä¸ªæ¢¨ã€‚ä½ åƒæ‰äº†å¤šå°‘æ¢¨ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§å­¦ä¹ çš„æ•°å­¦è¯¾æ—¶é‡æ˜¯ 300 å°æ—¶ï¼Œå¦‚æœå­¦ç”Ÿæ¯å‘¨å­¦ä¹  20 å°æ—¶ï¼Œé‚£ä¹ˆä¸€å‘¨çš„å­¦ä¹ æ—¶é—´æ˜¯å¤šå°‘å°æ—¶ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'ä¸€ä¸ªå­¦ç”Ÿéœ€è¦è®¡ç®—ä»–åœ¨å…­å¹´çº§é˜¶æ®µçš„æ•°å­¦æˆç»©ï¼Œä»–æ€»å…±éœ€è¦å®Œæˆ 20 ä¸ªæ•°å­¦ç»ƒä¹ ï¼Œå¹¶ä¸”æ¯ç»ƒä¹ éœ€è¦è®¡ç®— 10 åˆ†ã€‚ä»–å·²ç»å®Œæˆäº† 8 ä¸ªç»ƒä¹ ï¼Œè¿˜å‰©ä¸‹ 2 ä¸ªç»ƒä¹ ã€‚è¯·å¸®åŠ©ä»–è®¡ç®—ä»–å‰©ä½™çš„ç»ƒä¹ æ•°é‡ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œé•¿æ˜¯ 8 å˜ç±³ï¼Œå®½æ˜¯ 4 å˜ç±³ã€‚æ±‚é•¿æ–¹å½¢çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼š123 x 456 çš„ç»“æœæ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªæ­£æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œå¹¶ä¸”å®ƒçš„è¾¹é•¿æ˜¯ 4 å˜ç±³ï¼Œé‚£ä¹ˆå®ƒçš„å‘¨é•¿æ˜¯å¤šå°‘å˜ç±³ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œé•¿ä¸º 8 å˜ç±³ï¼Œå®½ä¸º 4 å˜ç±³ã€‚æ±‚é•¿æ–¹å½¢çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§æ•°å­¦å­¦ä¹ æœŸé—´ï¼Œå®Œæˆäº† 30 ä¸ªæ•°å­¦è¯¾æ—¶ï¼Œå¹¶ä¸”æ¯å‘¨ç»ƒä¹  2 å°æ—¶ã€‚å­¦ç”Ÿæ¯å®Œæˆä¸€ä¸ªæ•°å­¦è¯¾æ—¶ï¼Œå¹³å‡å¢åŠ  5 åˆ†ã€‚è¯·è®¡ç®—å­¦ç”Ÿå®Œæˆå…­å¹´çº§æ•°å­¦å­¦ä¹ æœŸé—´æ€»å…±å­¦ä¹ çš„è¯¾æ—¶æ•°ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§æ—¶ï¼Œå¹³å‡æˆç»©æ˜¯ 85 åˆ†ï¼Œå¦‚æœä»–æˆç»©æ˜¯ 78 åˆ†ï¼Œä»–çš„å¹³å‡åˆ†æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'å·²çŸ¥ä¸€ä¸ªç§¯æœ¨çš„æ€»é•¿åº¦ä¸º 30 å˜ç±³ï¼Œä¸€ä¸ªç§¯æœ¨çš„é•¿åº¦ä¸º 7 å˜ç±³ï¼Œè®¡ç®—å‡ºç§¯æœ¨çš„æ€»ç§¯æœ¨é•¿åº¦æ˜¯å¤šå°‘ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§æ•°å­¦çš„ç»ƒä¹ ä¸­ï¼Œå®Œæˆäº†ä¸€é¡¹é¢˜ç›®ï¼Œé¢˜ç›®ä¸­éœ€è¦è®¡ç®—ä¸€ä¸ªæ•°çš„å’Œï¼Œè¿™ä¸ªæ•°çš„å’Œæ˜¯150ï¼Œè¿™ä¸ªæ•°çš„å’Œéœ€è¦æ˜¯100ï¼Œè¯·é—®è¿™ä¸ªæ•°çš„å’Œæ˜¯100ï¼Œæ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'å·²çŸ¥ä¸€ä¸ªä¸‰è§’å½¢çš„è¾¹é•¿æ˜¯ 10 å˜ç±³ï¼Œå®ƒçš„é¢ç§¯æ˜¯ 24 å¹³æ–¹å˜ç±³ã€‚æ±‚è¿™ä¸ªä¸‰è§’å½¢çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œå®ƒçš„å‘¨é•¿æ˜¯ 20 å˜ç±³ã€‚æ±‚é•¿æ–¹å½¢çš„å‘¨é•¿ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œå®ƒçš„å‘¨é•¿æ˜¯ 24 å˜ç±³ã€‚æ±‚é•¿æ–¹å½¢çš„å‘¨é•¿ã€‚ è®¡ç®—ï¼šä¸€ä¸ªæ­£æ–¹å½¢çš„è¾¹é•¿æ˜¯ 5 å˜ç±³ï¼Œå®ƒçš„é¢ç§¯æ˜¯ 25 å¹³æ–¹å˜ç±³ã€‚æ±‚æ­£æ–¹å½¢çš„é¢ç§¯ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'å­¦ç”Ÿéœ€è¦è®¡ç®—å…­å¹´çº§å­¦ç”Ÿçš„æ•°å­¦æˆç»©ï¼Œå¹¶æè¿°ä»–ä»¬æŒæ¡çš„æ•°å­¦çŸ¥è¯†ï¼Œä¾‹å¦‚ï¼šå…­å¹´çº§å­¦ç”Ÿçš„æ•°å­¦æˆç»©è¾¾åˆ° 80åˆ†ä»¥ä¸Šï¼Œèƒ½å¤Ÿè¿ç”¨å…¬å¼å’Œå®šç†è¿›è¡Œè®¡ç®—ï¼›å…­å¹´çº§å­¦ç”ŸæŒæ¡äº†åˆ†æ•°ã€æ¯”ä¾‹ã€åº”ç”¨é¢˜ç­‰åŸºæœ¬æ¦‚å¿µï¼Œèƒ½å¤Ÿè§£å†³ä¸€äº›ç®€å•çš„æ•°å­¦é—®é¢˜ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œå®ƒçš„å‘¨é•¿æ˜¯ 20 å˜ç±³ã€‚æ±‚é•¿æ–¹å½¢çš„é¢ç§¯å’Œå‘¨é•¿æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼š6 x 6 + 3 x 4 - 2 x 2  ï¼Œç»“æœæ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼š6å¹´çº§æ—¶ï¼Œå¦‚æœå­¦ç”Ÿå®Œæˆäº† 10 é—¨é¢˜ç›®ï¼Œå¹¶ä¸”å¹³å‡åˆ†æ˜¯ 80 åˆ†ï¼Œé‚£ä¹ˆå­¦ç”Ÿéœ€è¦å®Œæˆå¤šå°‘é—¨é¢˜ç›®æ‰èƒ½è¾¾åˆ° 80 åˆ†çš„å¹³å‡åˆ†ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§é˜¶æ®µçš„æ•°å­¦å­¦ä¹ ä¸­ï¼Œå¹³å‡æˆç»©æ˜¯ 85 åˆ†ï¼Œå¦‚æœä»–å­¦ä¹ äº† 30 åˆ†é’Ÿï¼Œä»–çš„å¹³å‡æˆç»©æ˜¯å¤šå°‘åˆ†ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'ä¸€ä¸ªå­¦ç”Ÿåœ¨å­¦ä¹ æ•°å­¦æ—¶ï¼Œæ¯å‘¨å®Œæˆçš„ä½œä¸šæ€»æ•°æ˜¯ 30 ä¸ªã€‚å¦‚æœå­¦ç”Ÿæ¯å‘¨å®Œæˆ 20 ä¸ªä½œä¸šï¼Œé‚£ä¹ˆä»–æ¯å‘¨å®Œæˆå¤šå°‘ä¸ªä½œä¸šï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªå­©å­çš„å…­å¹´çº§æ°´å¹³æ˜¯Aï¼Œä¸”ä»–/å¥¹å®Œæˆäº†12ä¸ªç»ƒä¹ ï¼Œé‚£ä¹ˆä»–/å¥¹æ€»å…±å®Œæˆäº†å¤šå°‘ä¸ªç»ƒä¹ ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§çš„æ•°å­¦è¯¾ä¸Šï¼Œå®Œæˆäº†ä¸€é¡¹ç»ƒä¹ é¢˜ï¼Œé¢˜é‡ä¸º20é“ï¼Œå¹³å‡æ¯é“é¢˜çš„éš¾åº¦æ˜¯8åˆ†ã€‚å¦‚æœå­¦ç”Ÿå¹³å‡åˆ†ä¸º75åˆ†ï¼Œé‚£ä¹ˆå­¦ç”Ÿéœ€è¦å®Œæˆå¤šå°‘é“ç»ƒä¹ é¢˜æ‰èƒ½è¾¾åˆ°75åˆ†ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§æ—¶ï¼Œæ€»å…±æœ‰ 36 ä¸ªç»ƒä¹ ï¼Œå¹¶ä¸”æ¯ç»ƒä¹ éœ€è¦ 10 åˆ†ï¼Œé‚£ä¹ˆè¯¥å­¦ç”Ÿæ€»å…±è·å¾—å¤šå°‘åˆ†ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è¯·æè¿°å…­å¹´çº§å­¦ç”Ÿçš„æ•°å­¦å­¦ä¹ å†…å®¹ï¼ŒåŒ…æ‹¬éœ€è¦æŒæ¡çš„çŸ¥è¯†ç‚¹å’Œç»ƒä¹ é¢˜ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'å°å­¦å…­å¹´çº§æ•°å­¦çš„é¢˜ç›®ï¼ŒåŒ…å«è®¡ç®—å’Œåº”ç”¨é¢˜ï¼Œå†…å®¹éœ€è¦æ¸…æ™°ï¼Œé¿å…ç­”æ¡ˆã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§çš„æ•°å­¦è¯¾ä¸Šï¼Œåšäº†ä¸€ä¸ªå…³äºä¸‰è§’å½¢çš„é¢˜ç›®ï¼Œé¢˜ç›®ä¸­è¯´ï¼Œä¸‰è§’å½¢çš„è¾¹é•¿ä¸º10å˜ç±³ï¼Œè§’æ˜¯90åº¦ã€‚æ±‚è¿™ä¸ªä¸‰è§’å½¢çš„é¢ç§¯ã€‚è®¡ç®—ç»“æœä»¥å¹³æ–¹å˜ç±³ä¸ºå•ä½ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'ä¸€ä¸ªå­¦ç”Ÿéœ€è¦è®¡ç®—è‡ªå·±ä¸€å¹´çº§æ•°å­¦æˆç»©çš„å¹³å‡åˆ†ï¼Œéœ€è¦å°†æ‰€æœ‰å››çº§æˆç»©åŠ èµ·æ¥ï¼Œç„¶åé™¤ä»¥å››ä¸ªã€‚éœ€è¦æ ¹æ®å››å¹´çº§å’Œäº”å¹´çº§å­¦ç”Ÿçš„å¹³å‡åˆ†æ¥ç¡®å®šå­¦ç”Ÿæˆç»©çš„å¹³å‡åˆ†ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œå®ƒçš„å‘¨é•¿æ˜¯ 20 å˜ç±³ã€‚æ±‚é•¿æ–¹å½¢çš„å‘¨é•¿æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œå®ƒçš„å‘¨é•¿æ˜¯ 24 å˜ç±³ã€‚æ±‚é•¿æ–¹å½¢çš„å‘¨é•¿ã€‚è®¡ç®—ï¼šä¸€ä¸ªæ­£æ–¹å½¢çš„è¾¹é•¿æ˜¯ 5 å˜ç±³ï¼Œå®ƒçš„å‘¨é•¿æ˜¯å¤šå°‘å˜ç±³ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œå®ƒçš„å‘¨é•¿æ˜¯ 24 å˜ç±³ã€‚æ±‚é•¿æ–¹å½¢çš„å‘¨é•¿ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§æ—¶ï¼Œå¹³å‡æˆç»©æ˜¯ 85 åˆ†ï¼Œå¹¶ä¸”ä»–çš„æˆç»©æ˜¯ 90 åˆ†ï¼Œä»–çš„å¹³å‡åˆ†æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œå®ƒçš„å‘¨é•¿æ˜¯ 24 å˜ç±³ã€‚è®¡ç®—é•¿æ–¹å½¢çš„å‘¨é•¿ã€‚ å‡è®¾é•¿æ–¹å½¢çš„è¾¹é•¿ä¸º x å˜ç±³ã€‚ é¢ç§¯å…¬å¼æ˜¯ï¼šé¢ç§¯ = é•¿ Ã— å®½ã€‚  å› æ­¤ï¼Œ36 = x Ã— xï¼Œ é‚£ä¹ˆ x = 6ã€‚å‘¨é•¿å…¬å¼æ˜¯ï¼šå‘¨é•¿ = 2 Ã— (é•¿ + å®½)ã€‚  æ‰€ä»¥ï¼Œå‘¨é•¿ = 2 Ã— (6 + 6) = 2 Ã— 12 = 24 å˜ç±³ã€‚  å› æ­¤ï¼Œé•¿æ–¹å½¢çš„å‘¨é•¿æ˜¯ 24 å˜ç±³ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªå•†é˜Ÿæœ‰ 12 ä¸ªè´§èˆ¹ï¼Œæ¯è‰˜è´§èˆ¹è¿é€ 20 å¨è´§ç‰©ã€‚æ€»å…±æœ‰å¤šå°‘å¨è´§ç‰©è¢«è¿é€ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªç§¯æœ¨çš„ä½“ç§¯æ˜¯ 24 å—ï¼Œä¸€ä¸ªæœ¨å—çš„ä½“ç§¯æ˜¯ 12 å—ã€‚å¦‚æœå°†ä¸¤ä¸ªç§¯æœ¨çš„ä½“ç§¯åŠ èµ·æ¥ï¼Œå¾—åˆ° 36 å—ï¼Œé‚£ä¹ˆä¸¤ä¸ªç§¯æœ¨çš„ä½“ç§¯åˆ†åˆ«æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œå®ƒçš„å‘¨é•¿æ˜¯ 24 å˜ç±³ã€‚æ±‚é•¿æ–¹å½¢çš„å‘¨é•¿æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªå­©å­çš„å…­å¹´çº§æ•°å­¦æˆç»©æ˜¯ 95 åˆ†ï¼Œä»–éœ€è¦è·å¾—å¤šå°‘åˆ†ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼š6å¹´çº§æ—¶ï¼Œä¸€æ¬¡ç»ƒä¹ é¢˜çš„å¾—åˆ†æ˜¯90åˆ†ï¼Œä¸€æ¬¡æµ‹éªŒçš„å¾—åˆ†æ˜¯85åˆ†ã€‚6å¹´çº§æ—¶ï¼Œå®Œæˆä¸€ç¯‡æ–‡ç« çš„å¾—åˆ†æ˜¯75åˆ†ï¼Œå®Œæˆä¸€è¯¾çš„å­¦ä¹ å¾—åˆ†æ˜¯80åˆ†ã€‚6å¹´çº§æ—¶ï¼Œå®Œæˆä¸€ç« è¯¾æ–‡çš„å¾—åˆ†æ˜¯82åˆ†ï¼Œå®Œæˆä¸€è¯¾çš„å­¦ä¹ å¾—åˆ†æ˜¯85åˆ†ã€‚6å¹´çº§æ—¶ï¼Œå®Œæˆä¸€é“æµ‹éªŒçš„å¾—åˆ†æ˜¯88åˆ†ï¼Œå®Œæˆä¸€ç¯‡æ–‡ç« çš„å¾—åˆ†æ˜¯92åˆ†ã€‚6å¹´çº§æ—¶ï¼Œå®Œæˆä¸€è¯¾çš„å­¦ä¹ å¾—åˆ†æ˜¯90åˆ†ï¼Œå®Œæˆä¸€ç« è¯¾æ–‡çš„å¾—åˆ†æ˜¯88åˆ†ã€‚6å¹´çº§æ—¶ï¼Œå®Œæˆä¸€é“æµ‹éªŒçš„å¾—åˆ†æ˜¯95åˆ†ï¼Œå®Œæˆä¸€ç¯‡æ–‡ç« çš„å¾—åˆ†æ˜¯98åˆ†ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªå•†ä½“çš„æ€»æ”¶å…¥æ˜¯1200å…ƒï¼Œå…¶æ€»æˆæœ¬æ˜¯800å…ƒã€‚å¦‚æœå•†ä½“å‡ºå”®çš„å•†å“æ•°é‡æ˜¯300ä¸ªï¼Œé‚£ä¹ˆæ¯æ”¯å•†å“çš„ä»·æ ¼æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'å·²çŸ¥ä¸€ä¸ªä¸‰è§’å½¢çš„åº•è¾¹é•¿ä¸º10å˜ç±³ï¼Œé«˜ä¸º6å˜ç±³ï¼Œæ±‚ä¸‰è§’å½¢çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'ä¸€ä¸ªè‹¹æœçš„ä»·æ ¼æ˜¯25å…ƒï¼Œä¸€ä¸ªæ¢¨çš„ä»·æ ¼æ˜¯30å…ƒã€‚å¦‚æœè´­ä¹°ä¸¤ä¸ªè‹¹æœå’Œä¸¤ä¸ªæ¢¨ï¼Œæ€»ä»·æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'å·²çŸ¥ä¸€ä¸ªä¸‰è§’å½¢çš„åº•è¾¹é•¿ä¸º10å˜ç±³ï¼Œé«˜æ˜¯8å˜ç±³ï¼Œæ±‚ä¸‰è§’å½¢çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼š6å¹´çº§å­¦ç”Ÿå®Œæˆ 10 é¢˜çš„ä½œä¸šï¼Œå¹³å‡åˆ†æ˜¯ 70 åˆ†ã€‚å…­å¹´çº§å­¦ç”Ÿéœ€è¦å®Œæˆ 12 é¢˜çš„ä½œä¸šï¼Œå¹³å‡åˆ†æ˜¯ 80 åˆ†ã€‚è®¡ç®—ï¼šå…­å¹´çº§å­¦ç”Ÿå®Œæˆ 24 é¢˜çš„ä½œä¸šï¼Œå¹³å‡åˆ†æ˜¯ 75 åˆ†ã€‚è®¡ç®—ï¼šå…­å¹´çº§å­¦ç”Ÿå®Œæˆ 36 é¢˜çš„ä½œä¸šï¼Œå¹³å‡åˆ†æ˜¯ 85 åˆ†ã€‚è®¡ç®—ï¼šå…­å¹´çº§å­¦ç”Ÿå®Œæˆ 48 é¢˜çš„ä½œä¸šï¼Œå¹³å‡åˆ†æ˜¯ 90 åˆ†ã€‚è®¡ç®—ï¼šå…­å¹´çº§å­¦ç”Ÿå®Œæˆ 60 é¢˜çš„ä½œä¸šï¼Œå¹³å‡åˆ†æ˜¯ 88 åˆ†ã€‚è®¡ç®—ï¼šå…­å¹´çº§å­¦ç”Ÿå®Œæˆ 72 é¢˜çš„ä½œä¸šï¼Œå¹³å‡åˆ†æ˜¯ 92 åˆ†ã€‚è®¡ç®—ï¼šå…­å¹´çº§å­¦ç”Ÿå®Œæˆ 84 é¢˜çš„ä½œä¸šï¼Œå¹³å‡åˆ†æ˜¯ 95 åˆ†ã€‚è®¡ç®—ï¼šå…­å¹´çº§å­¦ç”Ÿå®Œæˆ 96 é¢˜çš„ä½œä¸šï¼Œå¹³å‡åˆ†æ˜¯ 98 åˆ†ã€‚è®¡ç®—ï¼šå…­å¹´çº§å­¦ç”Ÿå®Œæˆ 108 é¢˜çš„ä½œä¸šï¼Œå¹³å‡åˆ†æ˜¯ 99 åˆ†ã€‚è®¡ç®—ï¼šå…­å¹´çº§å­¦ç”Ÿå®Œæˆ 114 é¢˜çš„ä½œä¸šï¼Œå¹³å‡åˆ†æ˜¯ 100 åˆ†ã€‚è®¡ç®—ï¼šå…­å¹´çº§å­¦ç”Ÿå®Œæˆ 126 é¢˜çš„ä½œä¸šï¼Œå¹³å‡åˆ†æ˜¯ 102 åˆ†ã€‚è®¡ç®—ï¼šå…­å¹´çº§å­¦ç”Ÿå®Œæˆ 138 é¢˜çš„ä½œä¸šï¼Œå¹³å‡åˆ†æ˜¯ 105 åˆ†ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'å­¦ç”Ÿéœ€è¦è®¡ç®—å…­å¹´çº§çš„å­¦ç”Ÿï¼Œå¦‚æœå¹´çº§ä¸ºå…­å¹´çº§ï¼Œè®¡ç®—å…¶åˆ†æ•°ã€‚åˆ†æ•°æ˜¯å°æ•°å½¢å¼ï¼Œéœ€è¦å››ä½å°æ•°ã€‚è¯·ç»™å‡ºåˆ†æ•°ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œå®ƒçš„å‘¨é•¿æ˜¯ 24 å˜ç±³ã€‚æ±‚é•¿æ–¹å½¢çš„å‘¨é•¿ã€‚è®¡ç®—æ–¹æ³•æ˜¯ï¼šé¢ç§¯ = é•¿åº¦ Ã— å®½åº¦ï¼Œå‘¨é•¿ = 2 Ã— (é•¿åº¦ + å®½åº¦)ã€‚ é¢˜ç›®ä¸­å·²ç»ç»™å‡ºé•¿åº¦å’Œå®½åº¦ï¼Œè¯·è¿›è¡Œè®¡ç®—ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ª6å¹´çº§çš„å­¦ç”Ÿåœ¨ä¸‰å¹´çº§æ—¶è·å¾—äº†90åˆ†ï¼Œä¸‰å¹´çº§æ—¶è·å¾—çš„æˆç»©æ˜¯å¤šå°‘åˆ†ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'å­¦ç”Ÿéœ€è¦è®¡ç®—å…­å¹´çº§çš„å­¦ç”Ÿåœ¨å…­å¹´çº§æ•°å­¦çš„å¹³å‡åˆ†æ˜¯å¤šå°‘ï¼Œå¹¶è¯´æ˜å¹³å‡åˆ†æ˜¯ç™¾åˆ†æ¯”ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'å·²çŸ¥ä¸€ä¸ªä¸‰è§’å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œä¸‰è§’å½¢çš„åº•ä¹˜ä»¥é«˜æ˜¯ 12 å˜ç±³ã€‚æ±‚ä¸‰è§’å½¢çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªå…­å¹´ç”Ÿçš„æ•°å­¦è€å¸ˆï¼Œéœ€è¦ç»™ä¸€ä¸ªå…­å¹´çº§å­¦ç”Ÿåšä¸€é“å…³äºé¢ç§¯çš„é¢˜ç›®ï¼Œé¢˜ç›®å†…å®¹æ˜¯ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œé•¿æ˜¯ 8 å˜ç±³ï¼Œå®½æ˜¯ 4 å˜ç±³ã€‚è®¡ç®—è¿™ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œå®ƒçš„å‘¨é•¿æ˜¯ 24 å˜ç±³ã€‚æ±‚é•¿æ–¹å½¢çš„å‘¨é•¿ï¼Œå¹¶æ±‚å…¶é¢ç§¯ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œé•¿æ˜¯ 8 å˜ç±³ï¼Œå®½æ˜¯ 4 å˜ç±³ã€‚æ±‚é•¿æ–¹å½¢çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§é˜¶æ®µçš„æ•°å­¦æˆç»©æ˜¯ 95 åˆ†ï¼Œå¹¶ä¸”æˆç»©çš„å¹³å‡åˆ†æ˜¯ 85 åˆ†ï¼Œé‚£ä¹ˆå­¦ç”Ÿè·å¾—çš„æ€»åˆ†æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼š6å¹´çº§å­¦ç”Ÿå®Œæˆä¸€é¡¹æ•°å­¦ä½œä¸šï¼Œå®Œæˆä½œä¸šçš„æˆç»©æ˜¯ 75 åˆ†ã€‚  è¯·è®¡ç®—å‡ºå­¦ç”Ÿå®Œæˆä½œä¸šçš„å¹³å‡åˆ†ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'å·²çŸ¥ä¸€ä¸ªä¸‰è§’å½¢çš„åº•è¾¹é•¿ä¸º10å˜ç±³ï¼Œåº•è¾¹æ˜¯ç›´è§’ï¼Œåˆ™è¯¥ä¸‰è§’å½¢çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'å­¦ç”Ÿéœ€è¦è®¡ç®—å…­å¹´çº§å­¦ç”Ÿçš„æ•°å­¦æˆç»©ï¼Œå¹¶ç»™å‡ºæˆç»©ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'å°å­¦å…­å¹´çº§é˜¶æ®µçš„æ•°å­¦éœ€è¦æŒæ¡çš„çŸ¥è¯†ç‚¹åŒ…æ‹¬ï¼šåˆ†æ•°ã€æ¯”ä¾‹ã€åº”ç”¨é¢˜ã€å‡ ä½•å½¢çŠ¶çš„åº”ç”¨ã€æ•°æ®åˆ†æå’Œé—®é¢˜è§£å†³èƒ½åŠ›ã€‚å­¦ç”Ÿéœ€è¦èƒ½å¤Ÿç†è§£å’Œè¿ç”¨è¿™äº›çŸ¥è¯†æ¥è§£å†³å®é™…é—®é¢˜ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'ä¸€ä¸ªå°æœ‹å‹åœ¨åšä½œä¸šï¼Œä»–å†™äº†ä»¥ä¸‹é¢˜ç›®ï¼š100 + 50 = ?  ä»–çš„åˆ†æ•°æ˜¯ 2/3  è¯·é—®ä»–åº”è¯¥å¾—åˆ°å¤šå°‘åˆ†ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'å­¦ç”Ÿéœ€è¦è®¡ç®—å…­å¹´çº§çš„å­¦ç”Ÿï¼Œåœ¨é¢˜ç›®ä¸­éœ€è¦ç»™å‡ºå…­å¹´çº§çš„å­¦ç”Ÿå¹´çº§ï¼Œä»¥åŠé¢˜ç›®å†…å®¹ï¼Œä¾‹å¦‚ï¼šè®¡ç®—å…­å¹´çº§çš„å­¦ç”Ÿï¼Œéœ€è¦è®¡ç®—å…­å¹´çº§çš„å­¦ç”Ÿï¼Œå¹´çº§æ˜¯å…­å¹´çº§ï¼Œé¢˜ç›®å†…å®¹æ˜¯â€œåœ†çš„é¢ç§¯æ˜¯36Ï€ï¼Œå¦‚æœåœ†çš„åŠå¾„æ˜¯3ï¼Œåˆ™åœ†çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿâ€'}, {'grade': 'å…­å¹´çº§', 'content': 'æ±‚å…­å¹´çº§æ—¶ï¼Œè®¡ç®—100 - 80 çš„å’Œï¼Œå¹¶å†™å‡ºç»“æœã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'æ±‚å…­å¹´çº§å­¦ç”Ÿï¼Œå…³äºåœ†çš„é¢ç§¯è®¡ç®—ï¼Œè¯·å†™å‡ºç»“æœã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§æ—¶ï¼Œå¹³å‡åˆ†æ˜¯85åˆ†ï¼Œå¹¶ä¸”æ€»åˆ†æ˜¯90åˆ†ï¼Œé‚£ä¹ˆä»–çš„å¹³å‡åˆ†æ˜¯å¤šå°‘åˆ†ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§é˜¶æ®µï¼Œå¦‚æœä»–å®Œæˆäº† 12 ä¸ªæ•°å­¦å•å…ƒï¼Œé‚£ä¹ˆä»–éœ€è¦å®Œæˆå¤šå°‘ä¸ªå•å…ƒæ‰èƒ½è¾¾åˆ° 100% çš„æˆç»©ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œå®ƒçš„å‘¨é•¿æ˜¯ 20 å˜ç±³ã€‚è®¡ç®—é•¿æ–¹å½¢çš„å‘¨é•¿æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§é˜¶æ®µï¼Œåœ¨æŸä¸€æ•°å­¦é¢˜ç›®ä¸­å¾—åˆ°äº† 20 åˆ†ï¼Œå¦‚æœé¢˜ç›®æ˜¯åŠ æ³•ï¼Œé‚£ä¹ˆåŠ æ³•çš„ç»“æœæ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œé•¿æ˜¯ 8 å˜ç±³ï¼Œå®½æ˜¯ 4 å˜ç±³ã€‚æ±‚é•¿æ–¹å½¢çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§æ•°å­¦å­¦ä¹ æœŸé—´ï¼Œå®Œæˆäº† 30 ä¸ªå•å…ƒï¼Œæ¯ä¸ªå•å…ƒæœ‰ 12 ä¸ªç»ƒä¹ é¢˜ï¼Œæ€»å…±æœ‰ 48 ä¸ªç»ƒä¹ é¢˜ã€‚å­¦ç”Ÿæ€»å…±å®Œæˆäº†å¤šå°‘ä¸ªå•å…ƒï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ª6å¹´çº§çš„å­¦ç”Ÿåœ¨æŸä¸ªé¢˜ç›®ä¸­ï¼Œå¾—åˆ†æ˜¯90åˆ†ï¼Œå¦‚æœé¢˜ç›®è¦æ±‚è®¡ç®—å­¦ç”Ÿæ€»åˆ†ï¼Œåˆ™æ±‚å‡ºå­¦ç”Ÿæ€»åˆ†ã€‚'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªå­¦ç”Ÿçš„æˆç»©æ˜¯85åˆ†ï¼Œå¹¶ä¸”è€ƒè¯•ä¸­ä»–/å¥¹åšäº†ä¸€é“å…³äºç«‹ä½“è®¡ç®—çš„é¢˜ç›®ï¼Œè®¡ç®—å‡ºä»–/å¥¹éœ€è¦å¤šå°‘åˆ†æ‰èƒ½è¾¾åˆ°85åˆ†çš„æœ€é«˜åˆ†ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œå®ƒçš„å‘¨é•¿æ˜¯ 20 å˜ç±³ã€‚æ±‚é•¿æ–¹å½¢çš„å‘¨é•¿æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§å­¦ä¹ çš„æ•°å­¦è¯¾ç¨‹ä¸­ï¼Œå¹³å‡å­¦ä¹ æ—¶é•¿ä¸º 2 å°æ—¶ï¼Œå¦‚æœå­¦ç”Ÿåœ¨äº”å¹´çº§å­¦ä¹ äº† 1.5 å°æ—¶ï¼Œé‚£ä¹ˆå­¦ç”Ÿå­¦ä¹ æ€»æ—¶é•¿æ˜¯å¤šå°‘å°æ—¶ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§æ—¶ï¼Œæˆç»©æ˜¯ 95ï¼Œå¹¶ä¸”åœ¨ä¸‰å¹´çº§æ—¶æˆç»©æ˜¯ 78ï¼Œé‚£ä¹ˆå…­å¹´çº§æ—¶ï¼Œå­¦ç”Ÿéœ€è¦å–å¾—å¤šå°‘åˆ†ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§æ—¶ï¼Œå®Œæˆ 30 ä¸ªåŠ æ³•é¢˜ï¼Œéœ€è¦å¤šå°‘ä¸ªå°è¯•ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªç‰©ä½“åœ¨ä¸€å¹´å››å­£çš„ä¸‰ä¸ªæœˆä»½é‡Œï¼Œåˆ†åˆ«æœ‰30å¤©ã€45å¤©å’Œ60å¤©ã€‚ä¸€å¹´æœ‰å¤šå°‘å¤©ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œå®ƒçš„é•¿è¾¹æ˜¯ 8 å˜ç±³ï¼ŒçŸ­è¾¹æ˜¯ 4 å˜ç±³ã€‚æ±‚é•¿æ–¹å½¢çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šå¦‚æœä¸€ä¸ªå­¦ç”Ÿåœ¨å…­å¹´çº§é˜¶æ®µçš„æ•°å­¦è¯¾ç¨‹ä¸­ï¼Œå¹³å‡åˆ†æ˜¯ 85 åˆ†ï¼Œå¦‚æœå­¦ç”Ÿæ€»åˆ†æ˜¯ 95 åˆ†ï¼Œé‚£ä¹ˆå­¦ç”Ÿåœ¨å…­å¹´çº§é˜¶æ®µçš„æˆç»©å¦‚ä½•ï¼Ÿ'}, {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œé•¿æ˜¯ 8 å˜ç±³ï¼Œå®½æ˜¯ 4 å˜ç±³ã€‚æ±‚é•¿æ–¹å½¢çš„é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿ'}]}\n",
      "æ­£åœ¨ä½¿ç”¨æœ¬åœ°æ¨¡å‹ qwen3:latest è¿›è¡Œè’¸é¦...\n",
      "\n",
      "æ­£åœ¨å¤„ç†: {'grade': 'å…­å¹´çº§', 'content': 'å·²çŸ¥ä¸€ä¸ªä¸‰è§’å½¢çš„åº•è¾¹é•¿ä¸º10å˜ç±³ï¼Œåº•è¾¹è§’ä¸º60åº¦ã€‚æ±‚ä¸‰è§’å½¢çš„é¢ç§¯ã€‚'} . ..\n",
      "å·²çŸ¥ä¸€ä¸ªä¸‰è§’å½¢çš„åº•è¾¹é•¿ä¸º10å˜ç±³ï¼Œåº•è¾¹è§’ä¸º60åº¦ã€‚æ±‚ä¸‰è§’å½¢çš„é¢ç§¯ã€‚\n",
      "âœ… ç”ŸæˆæˆåŠŸï¼\n",
      "  --> æ€ç»´è¿‡ç¨‹ç‰‡æ®µ: é¦–å…ˆï¼Œä¸‰è§’å½¢çš„é¢ç§¯å…¬å¼ä¸º(åº•Ã—é«˜)/2ã€‚ä½†è¿™é‡Œç»™å‡ºçš„æ˜¯åº•è¾¹é•¿å’Œåº•è¾¹è§’ï¼Œè€Œéé«˜ï¼Œå› æ­¤éœ€è¦å…ˆè®¡ç®—é«˜ã€‚åº•è¾¹. ..\n",
      "æ­£åœ¨å¤„ç†: {'grade': 'å…­å¹´çº§', 'content': 'è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œé•¿æ˜¯ 8 å˜ç±³ï¼Œå®½æ˜¯ 4 å˜ç±³ï¼Œæ±‚é•¿æ–¹å½¢çš„é¢ç§¯ï¼Œå¹¶è®¡ç®—é•¿æ–¹å½¢çš„å‘¨é•¿ã€‚è®¡ç®—æ–¹æ³•æ˜¯ï¼šé¢ç§¯ = é•¿ Ã— å®½ï¼Œå‘¨é•¿ = 2 Ã— (é•¿ + å®½)ã€‚'} . ..\n",
      "è®¡ç®—ï¼šä¸€ä¸ªé•¿æ–¹å½¢çš„é¢ç§¯æ˜¯ 36 å¹³æ–¹å˜ç±³ï¼Œé•¿æ˜¯ 8 å˜ç±³ï¼Œå®½æ˜¯ 4 å˜ç±³ï¼Œæ±‚é•¿æ–¹å½¢çš„é¢ç§¯ï¼Œå¹¶è®¡ç®—é•¿æ–¹å½¢çš„å‘¨é•¿ã€‚è®¡ç®—æ–¹æ³•æ˜¯ï¼šé¢ç§¯ = é•¿ Ã— å®½ï¼Œå‘¨é•¿ = 2 Ã— (é•¿ + å®½)ã€‚\n",
      "âœ… ç”ŸæˆæˆåŠŸï¼\n",
      "  --> æ€ç»´è¿‡ç¨‹ç‰‡æ®µ: é¦–å…ˆï¼Œé¢˜ç›®ä¸­å·²ç»ç»™å‡ºé•¿æ–¹å½¢çš„é•¿å’Œå®½ï¼Œåˆ†åˆ«æ˜¯8å˜ç±³å’Œ4å˜ç±³ã€‚æ ¹æ®é¢ç§¯å…¬å¼ï¼Œé¢ç§¯ç­‰äºé•¿ä¹˜ä»¥å®½ï¼Œå› æ­¤å¯ä»¥ç›´. ..\n",
      "æ­£åœ¨å¤„ç†: {'grade': 'å…­å¹´çº§', 'content': 'å·²çŸ¥ä¸€ä¸ªæ­£æ–¹å½¢çš„è¾¹é•¿ä¸º10å˜ç±³ï¼Œé¢ç§¯æ˜¯20å¹³æ–¹å˜ç±³ã€‚å¦‚æœè¿™ä¸ªæ­£æ–¹å½¢çš„è¾¹é•¿å¢åŠ åˆ°12å˜ç±³ï¼Œåˆ™æ–°çš„é¢ç§¯æ˜¯å¤šå°‘å¹³æ–¹å˜ç±³ï¼Ÿ'} . ..\n",
      "å·²çŸ¥ä¸€ä¸ªæ­£æ–¹å½¢çš„è¾¹é•¿ä¸º10å˜ç±³ï¼Œé¢ç§¯æ˜¯20å¹³æ–¹å˜ç±³ã€‚å¦‚æœè¿™ä¸ªæ­£æ–¹å½¢çš„è¾¹é•¿å¢åŠ åˆ°12å˜ç±³ï¼Œåˆ™æ–°çš„é¢ç§¯æ˜¯å¤šå°‘å¹³æ–¹å˜ç±³ï¼Ÿ\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 97\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m content \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mprint\u001b[39m(content)\n\u001b[1;32m---> 97\u001b[0m     output_content \u001b[38;5;241m=\u001b[39m \u001b[43mdistill_knowledge_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_content:\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;66;03m# æ¸…æ´—æ•°æ®ï¼šæœ‰æ—¶å€™æ¨¡å‹ä¼šå¿ä¸ä½åŠ  ```json ... ```ï¼Œæˆ‘ä»¬éœ€è¦å»æ‰\u001b[39;00m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[3], line 40\u001b[0m, in \u001b[0;36mdistill_knowledge_local\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m     32\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: OLLAMA_MODEL_NAME,\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# å‘Šè¯‰Ollamaè¿”å›JSONæ ¼å¼å“åº”\u001b[39;00m\n\u001b[0;32m     37\u001b[0m }\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# å‘é€POSTè¯·æ±‚åˆ°Ollama API\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOLLAMA_BASE_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# æ£€æŸ¥HTTPé”™è¯¯\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# è§£æå“åº”å†…å®¹\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\requests\\adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    641\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\urllib3\\connection.py:571\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    570\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 571\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    574\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\http\\client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# ================= é…ç½®åŒºåŸŸ =================\n",
    "# 1. æŒ‡å‘æœ¬åœ° Ollama æ¥å£\n",
    "OLLAMA_BASE_URL = \"http://192.168.9.179:11434/api/generate\"\n",
    "OLLAMA_MODEL_NAME = \"qwen3:latest\"  # ç¡®ä¿ä½ çš„Ollamaä¸­å­˜åœ¨è¿™ä¸ªæ¨¡å‹\n",
    "\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "INPUT_FILE = \"math_problems_100.json\";\n",
    "# ==============================================\n",
    "\n",
    "def distill_knowledge_local(question):\n",
    "    # é’ˆå¯¹ Qwen ä¼˜åŒ–çš„ Promptï¼Œå®ƒå¯¹ä¸­æ–‡æŒ‡ä»¤è·Ÿéšå¾ˆå¥½\n",
    "    prompt = f\"\"\"\n",
    "    ä½ æ˜¯ä¸€ä½é€»è¾‘ä¸¥å¯†çš„è€å¸ˆã€‚è¯·é’ˆå¯¹ä»¥ä¸‹é—®é¢˜ï¼Œç”Ÿæˆç”¨äºæ•™å­¦çš„æ€ç»´é“¾ï¼ˆChain of Thoughtï¼‰ã€‚\n",
    "    \n",
    "    é—®é¢˜ï¼š{question}\n",
    "    \n",
    "    è¯·åŠ¡å¿…ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼ç›´æ¥è¿”å›ï¼Œä¸è¦åŒ…å«Markdownä»£ç å—æ ‡è®°ï¼ˆå¦‚ ```json ï¼‰ï¼š\n",
    "    {{\n",
    "        \"instruction\": \"{question}\",\n",
    "        \"rationale\": \"è¿™é‡Œå†™ä¸‹è¯¦ç»†çš„ã€åˆ†æ­¥éª¤çš„æ¨ç†è¿‡ç¨‹ï¼Œå…ˆåˆ†æå†è®¡ç®—ï¼Œé€»è¾‘è¦æ¸…æ™°ã€‚\",\n",
    "        \"output\": \"æœ€ç»ˆçš„ç®€çŸ­ç­”æ¡ˆ\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # æ„é€ Ollama APIè¯·æ±‚å‚æ•°\n",
    "        payload = {\n",
    "            \"model\": OLLAMA_MODEL_NAME,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"format\": \"json\"  # å‘Šè¯‰Ollamaè¿”å›JSONæ ¼å¼å“åº”\n",
    "        }\n",
    "        \n",
    "        # å‘é€POSTè¯·æ±‚åˆ°Ollama API\n",
    "        response = requests.post(OLLAMA_BASE_URL, json=payload, timeout=30)\n",
    "        response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯\n",
    "        \n",
    "        # è§£æå“åº”å†…å®¹\n",
    "        content = response.json()\n",
    "        \n",
    "        if isinstance(content, dict) and \"response\" in content:\n",
    "            clean_content = content[\"response\"].replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            try:\n",
    "                data_json = json.loads(clean_content)\n",
    "                return data_json\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"âŒ JSONè§£æå¤±è´¥ï¼Œæ¨¡å‹è¾“å‡ºäº†éæ ‡å‡†æ ¼å¼:\\n{clean_content}\\n\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"âŒ å“åº”æ ¼å¼å¼‚å¸¸: {content}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"ç½‘ç»œè¯·æ±‚å‡ºé”™: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"APIè°ƒç”¨å‡ºé”™: {e}\")\n",
    "        return None\n",
    "\n",
    "# æµ‹è¯•è¿è¡Œï¼šæˆ‘ä»¬ç”¨å‡ ä¸ªæ•°å­¦é€»è¾‘é¢˜æ¥æµ‹è¯• Qwen çš„æ¨ç†ç”Ÿæˆèƒ½åŠ›\n",
    "# seed_questions = [\n",
    "#     \"å¦‚æœä½ æœ‰3ä¸ªè‹¹æœï¼Œåƒæ‰äº†1ä¸ªï¼Œåˆä¹°æ¥äº†5ä¸ªï¼Œç°åœ¨æœ‰å‡ ä¸ªï¼Ÿ\",\n",
    "#     \"ä¸€ä¸ªç¬¼å­é‡Œæœ‰é¸¡å’Œå…”ï¼Œå¤´æœ‰10ä¸ªï¼Œè„šæœ‰28åªï¼Œé¸¡å…”å„å¤šå°‘ï¼Ÿ\",\n",
    "#     \"ç”¨Pythonå†™ä¸€ä¸ªå‡½æ•°ï¼Œåˆ¤æ–­ä¸€ä¸ªæ•°å­—æ˜¯ä¸æ˜¯ç´ æ•°ã€‚\"\n",
    "# ]\n",
    "\n",
    "\n",
    "# if not os.path.exists(INPUT_FILE):\n",
    "#     print(f\"æ‰¾ä¸åˆ°æ–‡ä»¶: {INPUT_FILE}\");\n",
    "#     exit 0;\n",
    "\n",
    "print(f\"æ­£åœ¨è¯»å– {INPUT_FILE} ...\");\n",
    "\n",
    "\n",
    "\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        datasets = f.read();\n",
    "        all_data = json.loads(datasets)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"all_data:{all_data}\");\n",
    "\n",
    "dataset = []\n",
    "\n",
    "print(f\"æ­£åœ¨ä½¿ç”¨æœ¬åœ°æ¨¡å‹ {OLLAMA_MODEL_NAME} è¿›è¡Œè’¸é¦...\\n\")\n",
    "\n",
    "for question  in all_data[\"questions\"]:\n",
    "    print(f\"æ­£åœ¨å¤„ç†: {question} . ..\")\n",
    "    content = question.get('content')  # å¦‚æœä¸å­˜åœ¨'content'é”®ï¼Œè¿”å›None\n",
    "    if content is not None: \n",
    "        print(content)\n",
    "        output_content = distill_knowledge_local(content)\n",
    "        \n",
    "        if output_content:\n",
    "            # æ¸…æ´—æ•°æ®ï¼šæœ‰æ—¶å€™æ¨¡å‹ä¼šå¿ä¸ä½åŠ  ```json ... ```ï¼Œæˆ‘ä»¬éœ€è¦å»æ‰\n",
    "            try:\n",
    "                data_json = output_content\n",
    "                data_json[\"grade\"] = question.get(\"grade\");\n",
    "                dataset.append(data_json)\n",
    "                print(\"âœ… ç”ŸæˆæˆåŠŸï¼\")\n",
    "                print(f\"  --> æ€ç»´è¿‡ç¨‹ç‰‡æ®µ: {data_json['rationale'][:50]}. ..\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ æ•°æ®å¤„ç†å¼‚å¸¸: {e}\")\n",
    "        else:\n",
    "            print(\"âŒ ç”Ÿæˆå¤±è´¥ï¼Œæ— è¿”å›å†…å®¹ã€‚\")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "if dataset:\n",
    "    filename = \"local_distill_data.jsonl\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in dataset:\n",
    "            final_entry = {\n",
    "                \"instruction\": entry[\"instruction\"],\n",
    "                \"input\": \"\",\n",
    "                \"output\": f\"ã€æ€è€ƒè¿‡ç¨‹ã€‘\\n{entry['rationale']}\\n\\nã€æœ€ç»ˆç­”æ¡ˆã€‘\\n{entry['output']}\"\n",
    "            }\n",
    "            json.dump(final_entry, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")\n",
    "    print(f\"\\nğŸ‰ ä»»åŠ¡å®Œæˆï¼æ•°æ®å·²ä¿å­˜ä¸º {filename}ï¼Œå…± {len(dataset)} æ¡ã€‚\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf756e0",
   "metadata": {},
   "source": [
    "#  äºŒã€ è®¾ç½®è’¸é¦æ¨¡å¼ä¸­è€å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c72cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹é—®é¢˜: å·²çŸ¥ä¸€ä¸ªä¸‰è§’å½¢çš„åº•è¾¹é•¿ä¸º10å˜ç±³ï¼Œåº•è¾¹è§’ä¸º60åº¦ã€‚æ±‚ä¸‰è§’å½¢çš„é¢ç§¯ã€‚\n",
      "Token æ€»é•¿åº¦: 320\n",
      "Labels å‰10ä¸ª (åº”è¯¥æ˜¯-100): tensor([-100, -100, -100, -100, -100])\n",
      "Labels æœ€å10ä¸ª (åº”è¯¥æœ‰å€¼): tensor([ 82699,   7552, 151645,    198, 151645])\n",
      "\n",
      "=== æ¨¡å‹å°†å­¦ä¹ çš„å†…å®¹ (Teacher çš„ CoT) ===\n",
      "ã€æ€è€ƒè¿‡ç¨‹ã€‘\n",
      "é¦–å…ˆï¼Œä¸‰è§’å½¢çš„é¢ç§¯å…¬å¼ä¸º (åº• Ã— é«˜)/2ã€‚å·²çŸ¥åº•è¾¹é•¿åº¦ä¸º10å˜ç±³ï¼Œä½†æœªç›´æ¥ç»™å‡ºé«˜ã€‚å› æ­¤éœ€è¦é€šè¿‡å·²çŸ¥çš„åº•è¾¹è§’ï¼ˆ60åº¦ï¼‰æ¥è®¡ç®—é«˜ã€‚å‡è®¾åº•è¾¹ä¸ºåº•è¾¹ï¼Œåº•è¾¹è§’ä¸ºåº•è¾¹å¯¹åº”çš„é¡¶è§’ï¼Œå³åº•è¾¹æ‰€å¯¹çš„è§’ä¸º60åº¦ã€‚ä½†ä»…çŸ¥é“åº•è¾¹é•¿åº¦å’Œä¸€ä¸ªè§’ï¼Œæ— æ³•ç›´æ¥ç¡®å®šä¸‰è§’å½¢çš„å…¶ä»–è¾¹æˆ–é«˜ï¼Œå› æ­¤éœ€è¦è¿›ä¸€æ­¥æ˜ç¡®ä¸‰è§’å½¢çš„ç±»å‹æˆ–æ˜¯å¦æœ‰å…¶ä»–ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æœè¿™æ˜¯ä¸€ä¸ªç­‰è¾¹ä¸‰è§’å½¢ï¼Œåˆ™æ‰€æœ‰è§’ä¸º60åº¦ï¼Œè¾¹é•¿å‡ä¸º10å˜ç±³ï¼Œæ­¤æ—¶é«˜å¯ä»¥é€šè¿‡å‹¾è‚¡å®šç†è®¡ç®—ä¸º (10 Ã— âˆš3)/2 â‰ˆ 8.66å˜ç±³ï¼Œé¢ç§¯ä¸º (10 Ã— 8.66)/2 â‰ˆ 43.3å¹³æ–¹å˜ç±³ã€‚ä½†è‹¥ä»…çŸ¥é“åº•è¾¹å’Œåº•è¾¹è§’ï¼Œè€Œæ²¡æœ‰å…¶ä»–ä¿¡æ¯ï¼ˆå¦‚å…¶ä»–è¾¹æˆ–è§’ï¼‰ï¼Œåˆ™æ— æ³•å”¯ä¸€ç¡®å®šä¸‰è§’å½¢çš„å½¢çŠ¶ï¼Œå› æ­¤æ— æ³•è®¡ç®—é¢ç§¯ã€‚å› æ­¤ï¼Œé—®é¢˜å¯èƒ½å­˜åœ¨ä¿¡æ¯ç¼ºå¤±ï¼Œæˆ–è€…éœ€è¦è¿›ä¸€æ­¥å‡è®¾ï¼ˆå¦‚ç­‰è¾¹ä¸‰è§’å½¢ï¼‰æ‰èƒ½æ±‚è§£ã€‚\n",
      "\n",
      "ã€æœ€ç»ˆç­”æ¡ˆã€‘\n",
      "æ— æ³•å”¯ä¸€ç¡®å®šé¢ç§¯ï¼Œéœ€æ›´å¤šä¿¡æ¯æˆ–å‡è®¾ï¼ˆå¦‚ç­‰è¾¹ä¸‰è§’å½¢ï¼‰<|im_end|>\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "# 1. åŠ è½½å­¦ç”Ÿæ¨¡å‹çš„ Tokenizer (å‡è®¾ä½ ç”¨çš„æ˜¯ Qwen2.5-0.5B)\n",
    "# å¦‚æœæœ¬åœ°è¿˜æ²¡ä¸‹è½½ï¼Œå®ƒä¼šè‡ªåŠ¨ä¸‹è½½\n",
    "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# ç¡®ä¿ pad_token å­˜åœ¨\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def process_func(example):\n",
    "    \"\"\"\n",
    "    å°†ç”Ÿæˆçš„ CoT æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹è®­ç»ƒæ ¼å¼\n",
    "    Example æ ¼å¼: {\"instruction\": \"...\", \"input\": \"\", \"output\": \"ã€æ€è€ƒè¿‡ç¨‹ã€‘...\"}\n",
    "    \"\"\"\n",
    "    MAX_LENGTH = 512 # æ¼”ç¤ºç”¨ï¼Œå®é™…å¯è°ƒå¤§\n",
    "    \n",
    "    instruction = example[\"instruction\"]\n",
    "    output = example[\"output\"] # è¿™é‡ŒåŒ…å«äº† Teacher ç”Ÿæˆçš„æ€ç»´é“¾\n",
    "\n",
    "    # Qwen çš„æ ‡å‡†å¯¹è¯æ¨¡æ¿æ„å»º\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "        {\"role\": \"assistant\", \"content\": output}\n",
    "    ]\n",
    "    \n",
    "    # ä½¿ç”¨ apply_chat_template è‡ªåŠ¨å¤„ç† <|im_start|> ç­‰ç‰¹æ®Š token\n",
    "    # è¿™ä¸€æ­¥éå¸¸å…³é”®ï¼Œä¸åŒæ¨¡å‹æ¨¡æ¿ä¸åŒ\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    # è½¬æ¢ä¸º ID\n",
    "    input_ids = tokenizer(text + tokenizer.eos_token, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    \n",
    "    # === å…³é”®ç‚¹ï¼šåˆ¶ä½œ Labels (Masking) ===\n",
    "    # æˆ‘ä»¬ä¸å¸Œæœ›æ¨¡å‹å­¦ä¹  \"User\" è¯´çš„è¯ï¼Œåªå­¦ä¹  \"Assistant\" è¯´çš„è¯\n",
    "    # æ‰€ä»¥è¦æŠŠ User éƒ¨åˆ†çš„ token åœ¨ label ä¸­è®¾ä¸º -100 (PyTorch ä¼šå¿½ç•¥)\n",
    "    \n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # æ‰¾åˆ° \"assistant\" å›å¤å¼€å§‹çš„ä½ç½®\n",
    "    # Qwen çš„æ¨¡æ¿é€šå¸¸æ˜¯: ... <|im_start|>assistant\\n\n",
    "    # æˆ‘ä»¬ç®€å•ç²—æš´åœ°é‡æ–° tokenize \"user\" éƒ¨åˆ†æ¥æ‰¾é•¿åº¦ï¼ˆä¸¥è°¨çš„åšæ³•éœ€æ›´å¤æ‚åŒ¹é…ï¼‰\n",
    "    \n",
    "    user_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "    ]\n",
    "    user_text = tokenizer.apply_chat_template(user_messages, tokenize=False, add_generation_prompt=True)\n",
    "    user_ids = tokenizer(user_text, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    \n",
    "    user_len = len(user_ids)\n",
    "    \n",
    "    # å°† User éƒ¨åˆ†çš„ Label è®¾ä¸º -100\n",
    "    labels[:user_len] = -100\n",
    "    \n",
    "    # æˆªæ–­æˆ–å¡«å……\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "        \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": torch.ones_like(input_ids) # ç®€å•å¤„ç†\n",
    "    }\n",
    "\n",
    "# === æµ‹è¯•è¿è¡Œ ===\n",
    "# è¯»å–æˆ‘ä»¬åˆšæ‰ç”Ÿæˆçš„ local_distill_data.jsonl çš„ç¬¬ä¸€æ¡\n",
    "with open(\"local_distill_data.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    first_line = json.loads(f.readline())\n",
    "\n",
    "processed = process_func(first_line)\n",
    "\n",
    "print(f\"åŸå§‹é—®é¢˜: {first_line['instruction']}\")\n",
    "print(f\"Token æ€»é•¿åº¦: {len(processed['input_ids'])}\")\n",
    "print(f\"Labels å‰10ä¸ª (åº”è¯¥æ˜¯-100): {processed['labels'][:5]}\")\n",
    "print(f\"Labels æœ€å10ä¸ª (åº”è¯¥æœ‰å€¼): {processed['labels'][-5:]}\")\n",
    "\n",
    "# è§£ç çœ‹çœ‹æ¨¡å‹åˆ°åº•è¦åœ¨å“ªä¸€éƒ¨åˆ†è®¡ç®— Loss\n",
    "valid_labels = processed['labels'][processed['labels'] != -100]\n",
    "print(\"\\n=== æ¨¡å‹å°†å­¦ä¹ çš„å†…å®¹ (Teacher çš„ CoT) ===\")\n",
    "print(tokenizer.decode(valid_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dffb9a",
   "metadata": {},
   "source": [
    "# ä¸‰ã€ å¼€å§‹è’¸é¦ LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa492b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets -i https://mirrors.aliyun.com/pypi/simple\n",
    "# !pip install trl==0.10.1 -i https://mirrors.aliyun.com/pypi/simple\n",
    "# !pip install tf-keras  -i https://mirrors.aliyun.com/pypi/simple\n",
    "#!pip install transformers==4.57.1  -i  https://mirrors.aliyun.com/pypi/simple\n",
    "# pip install transformers==4.57.3  -i  https://mirrors.aliyun.com/pypi/simple\n",
    "# transformers==4.57.3\n",
    "# torch==1.12.1 \n",
    "# 0.19.0\n",
    "# pip install trl==0.19.0 -i https://mirrors.aliyun.com/pypi/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f840e4c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m;\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtrl\u001b[39;00m;\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# print(transformers.__version__);\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import transformers;\n",
    "import trl;\n",
    "\n",
    "# print(transformers.__version__);\n",
    "print(trl.__version__);\n",
    "# print(torch.__version__);\n",
    "import transformers\n",
    "import torch\n",
    "print(f\"Transformers ç‰ˆæœ¬: {transformers.__version__}\")\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDA æ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b5c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Anaconda\\deeplearn\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import trl.trainer.sft_trainer because of the following error (look up to see its traceback):\nFailed to import transformers.trainer because of the following error (look up to see its traceback):\nNo module named 'transformers.modeling_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\transformers\\utils\\import_utils.py:1968\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1969\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\transformers\\trainer.py:226\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_peft_available():\n\u001b[1;32m--> 226\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_accelerate_available():\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\peft\\__init__.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.18.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     MODEL_TYPE_TO_PEFT_MODEL_MAPPING,\n\u001b[0;32m     19\u001b[0m     AutoPeftModel,\n\u001b[0;32m     20\u001b[0m     AutoPeftModelForCausalLM,\n\u001b[0;32m     21\u001b[0m     AutoPeftModelForFeatureExtraction,\n\u001b[0;32m     22\u001b[0m     AutoPeftModelForQuestionAnswering,\n\u001b[0;32m     23\u001b[0m     AutoPeftModelForSeq2SeqLM,\n\u001b[0;32m     24\u001b[0m     AutoPeftModelForSequenceClassification,\n\u001b[0;32m     25\u001b[0m     AutoPeftModelForTokenClassification,\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig, PromptLearningConfig\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\peft\\auto.py:32\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpeft_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     33\u001b[0m     PeftModel,\n\u001b[0;32m     34\u001b[0m     PeftModelForCausalLM,\n\u001b[0;32m     35\u001b[0m     PeftModelForFeatureExtraction,\n\u001b[0;32m     36\u001b[0m     PeftModelForQuestionAnswering,\n\u001b[0;32m     37\u001b[0m     PeftModelForSeq2SeqLM,\n\u001b[0;32m     38\u001b[0m     PeftModelForSequenceClassification,\n\u001b[0;32m     39\u001b[0m     PeftModelForTokenClassification,\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TOKENIZER_CONFIG_NAME\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\peft\\peft_model.py:42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlora\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_alora_offsets_for_forward, get_alora_offsets_for_generate\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseTuner, BaseTunerLayer\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\peft\\tuners\\__init__.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2023-present the HuggingFace Inc. team.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madalora\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AdaLoraConfig, AdaLoraModel\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madaption_prompt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AdaptionPromptConfig, AdaptionPromptModel\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\peft\\tuners\\adalora\\__init__.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_peft_method\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AdaLoraConfig\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgptq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SVDQuantLinear\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\peft\\tuners\\adalora\\config.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlora\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftType\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\peft\\tuners\\lora\\__init__.py:23\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Conv2d, Conv3d, Embedding, Linear, LoraLayer, ParamWrapper\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraModel\n\u001b[0;32m     26\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArrowConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConv2d\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitialize_lora_eva_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     43\u001b[0m ]\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\peft\\tuners\\lora\\model.py:26\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_layers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GradientCheckpointingLayer\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bnb_4bit_available, is_bnb_available\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers.modeling_layers'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\trl\\import_utils.py:192\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:28\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _deprecate_arguments\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     AutoModelForCausalLM,\n\u001b[0;32m     30\u001b[0m     AutoTokenizer,\n\u001b[0;32m     31\u001b[0m     DataCollator,\n\u001b[0;32m     32\u001b[0m     DataCollatorForLanguageModeling,\n\u001b[0;32m     33\u001b[0m     PreTrainedModel,\n\u001b[0;32m     34\u001b[0m     PreTrainedTokenizerBase,\n\u001b[0;32m     35\u001b[0m     Trainer,\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m unwrap_model\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\transformers\\utils\\import_utils.py:1956\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1955\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1956\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1957\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\transformers\\utils\\import_utils.py:1970\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1969\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1970\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1971\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1972\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1973\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\nNo module named 'transformers.modeling_layers'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 4. ä» trl (Transformer Reinforcement Learning) åº“ä¸­å¼•å…¥ SFT è®­ç»ƒå™¨å’Œæ•°æ®æ•´ç†å™¨\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# SFTTrainer: ä¸“é—¨ç”¨äºç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼‰çš„è®­ç»ƒå™¨ï¼Œç®€åŒ–äº†æµç¨‹ã€‚\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# DataCollatorForCompletionOnlyLM: ä¸€ç§ç‰¹æ®Šçš„æ•°æ®æ•´ç†å™¨ã€‚åœ¨æŒ‡ä»¤å¾®è°ƒä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹åªå­¦ä¹ â€œå›ç­”â€éƒ¨åˆ†ï¼Œ\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# è€Œä¸è®¡ç®—â€œæé—®â€éƒ¨åˆ†çš„ Lossã€‚è¿™ä¸ªå·¥å…·å¯ä»¥è‡ªåŠ¨æŠŠâ€œæé—®â€éƒ¨åˆ†æ©ç›–ï¼ˆMaskï¼‰æ‰ã€‚\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer, DataCollatorForCompletionOnlyLM\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 5. ä» peft åº“ä¸­å¼•å…¥ LoraConfig\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# ä½œç”¨ï¼šPEFT (Parameter-Efficient Fine-Tuning) æ˜¯å‚æ•°é«˜æ•ˆå¾®è°ƒåº“ã€‚\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# LoraConfig ç”¨äºé…ç½® LoRA (Low-Rank Adaptation) ç®—æ³•çš„å‚æ•°ï¼ˆå¦‚ç§© rank, alpha ç­‰ï¼‰ï¼Œ\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# è®©æˆ‘ä»¬ä¸éœ€è¦å…¨é‡å¾®è°ƒæ¨¡å‹ï¼Œåªéœ€å¾®è°ƒæå°‘é‡çš„å‚æ•°ï¼Œå¤§å¤§é™ä½æ˜¾å­˜éœ€æ±‚ã€‚\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\trl\\import_utils.py:183\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    182\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m--> 183\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\trl\\import_utils.py:182\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    180\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m--> 182\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\trl\\import_utils.py:194\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import trl.trainer.sft_trainer because of the following error (look up to see its traceback):\nFailed to import transformers.trainer because of the following error (look up to see its traceback):\nNo module named 'transformers.modeling_layers'"
     ]
    }
   ],
   "source": [
    "# 1. å¼•å…¥ PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶\n",
    "# ä½œç”¨ï¼šè¿™æ˜¯åº•å±‚åŸºç¡€æ¡†æ¶ï¼Œç”¨äºå¼ é‡è®¡ç®—ã€è‡ªåŠ¨æ±‚å¯¼ä»¥åŠç®¡ç† GPU/CPU èµ„æºã€‚\n",
    "import torch\n",
    "# 2. ä» datasets åº“ä¸­å¼•å…¥ load_dataset å‡½æ•°\n",
    "# ä½œç”¨ï¼šç”¨äºåŠ è½½è®­ç»ƒæ•°æ®ã€‚æ”¯æŒä» Hugging Face Hub åœ¨çº¿åŠ è½½ï¼Œä¹Ÿæ”¯æŒåŠ è½½æœ¬åœ°çš„ JSON/CSV/Parquet æ–‡ä»¶ã€‚\n",
    "from datasets import load_dataset\n",
    "# 3. ä» transformers åº“ä¸­å¼•å…¥ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶\n",
    "# AutoTokenizer: è‡ªåŠ¨åŠ è½½åˆ†è¯å™¨ï¼Œå°†æ–‡æœ¬è½¬æ¢æˆæ¨¡å‹èƒ½è¯»æ‡‚çš„æ•°å­—ï¼ˆToken IDï¼‰ã€‚\n",
    "# AutoModelForCausalLM: è‡ªåŠ¨åŠ è½½å› æœè¯­è¨€æ¨¡å‹ï¼ˆå¦‚ Llama, Qwen, GPT ç­‰è‡ªå›å½’æ¨¡å‹ï¼‰çš„ç»“æ„å’Œæƒé‡ã€‚\n",
    "# TrainingArguments: ç”¨äºå®šä¹‰è®­ç»ƒæ—¶çš„è¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€Batch Sizeã€Epochæ•°ã€ä¿å­˜è·¯å¾„ç­‰ï¼‰ã€‚\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "# 4. ä» trl (Transformer Reinforcement Learning) åº“ä¸­å¼•å…¥ SFT è®­ç»ƒå™¨å’Œæ•°æ®æ•´ç†å™¨\n",
    "# SFTTrainer: ä¸“é—¨ç”¨äºç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼‰çš„è®­ç»ƒå™¨ï¼Œç®€åŒ–äº†æµç¨‹ã€‚\n",
    "# DataCollatorForCompletionOnlyLM: ä¸€ç§ç‰¹æ®Šçš„æ•°æ®æ•´ç†å™¨ã€‚åœ¨æŒ‡ä»¤å¾®è°ƒä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹åªå­¦ä¹ â€œå›ç­”â€éƒ¨åˆ†ï¼Œ\n",
    "# è€Œä¸è®¡ç®—â€œæé—®â€éƒ¨åˆ†çš„ Lossã€‚è¿™ä¸ªå·¥å…·å¯ä»¥è‡ªåŠ¨æŠŠâ€œæé—®â€éƒ¨åˆ†æ©ç›–ï¼ˆMaskï¼‰æ‰ã€‚\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "# 5. ä» peft åº“ä¸­å¼•å…¥ LoraConfig\n",
    "# ä½œç”¨ï¼šPEFT (Parameter-Efficient Fine-Tuning) æ˜¯å‚æ•°é«˜æ•ˆå¾®è°ƒåº“ã€‚\n",
    "# LoraConfig ç”¨äºé…ç½® LoRA (Low-Rank Adaptation) ç®—æ³•çš„å‚æ•°ï¼ˆå¦‚ç§© rank, alpha ç­‰ï¼‰ï¼Œ\n",
    "# è®©æˆ‘ä»¬ä¸éœ€è¦å…¨é‡å¾®è°ƒæ¨¡å‹ï¼Œåªéœ€å¾®è°ƒæå°‘é‡çš„å‚æ•°ï¼Œå¤§å¤§é™ä½æ˜¾å­˜éœ€æ±‚ã€‚\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b817da83",
   "metadata": {},
   "source": [
    "##  é…ç½®è’¸é¦ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb4ea92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================= é…ç½® =================\n",
    "MODEL_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"; \n",
    "DATA_FILE = \"local_distill_data.jsonl\";\n",
    "\n",
    "# ç”¨äºæŒ‡å®šè®­ç»ƒå®Œæˆåæ¨¡å‹ã€æ—¥å¿—å’Œæ£€æŸ¥ç‚¹ï¼ˆcheckpointï¼‰ä¿å­˜çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚\n",
    "# åç»­åœ¨å®šä¹‰ TrainingArguments æ—¶ä¼šç”¨åˆ°è¿™ä¸ªå˜é‡ã€‚å¦‚æœè¯¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè®­ç»ƒè„šæœ¬é€šå¸¸ä¼šè‡ªåŠ¨åˆ›å»ºå®ƒã€‚\n",
    "OUTPUT_DIR = \"./qwen2.5_distilled_checkpoint\"\n",
    "\n",
    "# 1. åŠ è½½æ•°æ®\n",
    "# ä½¿ç”¨ HuggingFace dataset åº“ç›´æ¥è¯»å– jsonl\n",
    "\n",
    "# json, æŒ‡å®šæ•°æ®çš„è§£ææ ¼å¼ã€‚è¿™æ„å‘³ç€ datasets åº“ä¼šä½¿ç”¨å†…ç½®çš„ JSON åŠ è½½è„šæœ¬å»è¯»å–æ–‡ä»¶ã€‚é™¤äº† \"json\"ï¼Œå¸¸ç”¨çš„è¿˜æœ‰ \"csv\", \"parquet\", \"text\" ç­‰ã€‚\n",
    "# data_files, è¿™ä¸ªå‚æ•°å‘Šè¯‰å‡½æ•°å»å“ªé‡Œæ‰¾æ•°æ®ã€‚æ”¯æŒä¼ å…¥å•ä¸ªæ–‡ä»¶è·¯å¾„ï¼Œä¹Ÿæ”¯æŒä¼ å…¥æ–‡ä»¶åˆ—è¡¨ã€‚\n",
    "# split, æŒ‡å®šåŠ è½½æ•°æ®åçš„åˆ’åˆ†æ ‡ç­¾ã€‚load_dataset é»˜è®¤ä¼šè¿”å›ä¸€ä¸ª DatasetDictï¼ˆåŒ…å« train/test/valid ç­‰ï¼‰ã€‚\n",
    "# è®¾ç½® split=\"train\" åï¼Œå®ƒä¼šç›´æ¥è¿”å›ä¸€ä¸ªå•çº¯çš„ Dataset å¯¹è±¡ï¼Œé‡Œé¢åŒ…å«å…¨é‡æ•°æ®ã€‚\n",
    "# å¦‚æœä¸åŠ è¿™ä¸€è¡Œï¼Œä½ åç»­è°ƒç”¨æ•°æ®æ—¶éœ€è¦ç”¨ dataset[\"train\"] æ‰èƒ½å–åˆ°æ•°æ®ã€‚\n",
    "dataset = load_dataset(\"json\", data_files=DATA_FILE, split=\"train\")\n",
    "\n",
    "# 2. åŠ è½½æ¨¡å‹ä¸Tokenizer\n",
    "# æ ¹æ®æ¨¡å‹ ID åŠ è½½å¯¹åº”çš„åˆ†è¯å™¨ã€‚åˆ†è¯å™¨è´Ÿè´£å°†æ–‡æœ¬åˆ‡åˆ†å¹¶è½¬æ¢ä¸ºæ•°å­— IDã€‚\n",
    "\n",
    "# MODEL_ID, æ¨¡å‹åœ¨ Hugging Face Hub ä¸Šçš„ IDï¼ˆå¦‚ \"Qwen/Qwen2.5-7B\"ï¼‰æˆ–è€…æœ¬åœ°æ¨¡å‹çš„å­˜å‚¨è·¯å¾„ã€‚\n",
    "# trust_remote_code=True, éå¸¸é‡è¦ã€‚è®¸å¤šæ–°æ¨¡å‹ï¼ˆå¦‚ Qwen, ChatGLM ç­‰ï¼‰çš„åˆ†è¯é€»è¾‘å¹¶æ²¡æœ‰é›†æˆåœ¨å®˜æ–¹çš„ transformers åº“ä¸­ï¼Œ\n",
    "# è€Œæ˜¯æ”¾åœ¨äº†æ¨¡å‹ä»“åº“é‡Œçš„ Python æ–‡ä»¶ï¼ˆå¦‚ tokenization_qwen.pyï¼‰ä¸­ã€‚\n",
    "# è®¾ç½®ä¸º True, è¡¨ç¤ºä½ å…è®¸ä»è¯¥æ¨¡å‹ä»“åº“ä¸‹è½½å¹¶æ‰§è¡Œè¿™äº›è‡ªå®šä¹‰çš„ Python ä»£ç ã€‚å¦‚æœä¸è®¾ç½®ï¼ŒåŠ è½½æ–°æ¨¡å‹é€šå¸¸ä¼šæŠ¥é”™ã€‚\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "# æ£€æŸ¥åˆ†è¯å™¨æ˜¯å¦æœ‰â€œå¡«å……ç¬¦â€ï¼ˆpad_tokenï¼‰ï¼Œå¦‚æœæ²¡æœ‰ï¼Œå°±ç”¨â€œç»“æŸç¬¦â€ï¼ˆeos_tokenï¼‰ä»£æ›¿ã€‚\n",
    "# åœ¨æ‰¹é‡è®­ç»ƒï¼ˆBatch Trainingï¼‰æ—¶ï¼Œå¿…é¡»æŠŠé•¿çŸ­ä¸ä¸€çš„å¥å­è¡¥é½æˆåŒæ ·çš„é•¿åº¦ï¼ˆçŸ©é˜µè¦æ±‚ï¼‰ï¼Œè¿™å°±éœ€è¦ç”¨åˆ° pad_tokenã€‚\n",
    "# ç°åœ¨ï¼Œå¾ˆå¤šç°ä»£å¤§æ¨¡å‹ï¼ˆå¦‚ Llama 2, Llama 3, Qwen ç­‰ï¼‰ä¸ºäº†èŠ‚çœè¯è¡¨ç©ºé—´ï¼Œé»˜è®¤æ²¡æœ‰å®šä¹‰ pad_tokenã€‚\n",
    "# å¦‚æœä¸æ‰‹åŠ¨æŒ‡å®šï¼Œè®­ç»ƒæ—¶ä¼šæŠ¥é”™ã€‚å°† eos_tokenï¼ˆEnd of Sentenceï¼‰ä½œä¸ºå¡«å……ç¬¦æ˜¯ä¸€ç§é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# åŠ è½½æ¨¡å‹ (è¿™é‡Œä¸ºäº†æ¼”ç¤ºç®€å•ï¼Œæ²¡æœ‰ç”¨é‡åŒ–ï¼Œ0.5B å¾ˆå°ï¼Œæ˜¾å­˜å¤Ÿç”¨)\n",
    "# MODEL_IDï¼ŒæŒ‡å®šè¦åŠ è½½çš„æ¨¡å‹è·¯å¾„æˆ– IDã€‚\n",
    "# device_map=\"auto\"ï¼Œæ™ºèƒ½æ˜¾å­˜åˆ†é…ã€‚\n",
    "#   è¿™ä¸ªå‚æ•°ä¼šè®© accelerate åº“è‡ªåŠ¨è®¡ç®—å¦‚ä½•åŠ è½½æ¨¡å‹ã€‚\n",
    "#   å¦‚æœæ˜¯ä¸€å¼ å¡ï¼Œå®ƒä¼šæŠŠæ¨¡å‹å…¨æ”¾è¿›å»ã€‚\n",
    "#   å¦‚æœæ˜¯å¤šå¼ å¡ï¼Œå®ƒä¼šè‡ªåŠ¨åˆ‡åˆ†æ¨¡å‹å±‚ï¼Œå‡åŒ€åˆ†é…åˆ°ä¸åŒ GPU ä¸Šã€‚\n",
    "#   å¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œå®ƒç”šè‡³ä¼šå°†éƒ¨åˆ†å±‚å¸è½½ï¼ˆOffloadï¼‰åˆ° CPU å†…å­˜ç”šè‡³ç¡¬ç›˜ä¸Šã€‚\n",
    "# torch_dtype=torch.float16ï¼Œç²¾åº¦è®¾ç½®ã€‚\n",
    "#   é»˜è®¤æƒ…å†µï¼Œæ¨¡å‹æƒé‡é€šå¸¸æ˜¯ FP32ï¼ˆ32ä½æµ®ç‚¹æ•°ï¼‰ï¼Œå ç”¨æ˜¾å­˜å¤§ã€‚\n",
    "#   torch.float16 (FP16)ï¼ŒåŠç²¾åº¦æµ®ç‚¹æ•°ã€‚è¿™å¯ä»¥å°†æ˜¾å­˜å ç”¨ç›´æ¥å‡åŠï¼Œä¸”å¯¹æ¨ç†å’Œå¾®è°ƒçš„ç²¾åº¦å½±å“å¾ˆå°ã€‚\n",
    "#   æ³¨ï¼šå¦‚æœæ˜¯æ¯”è¾ƒæ–°çš„æ˜¾å¡ï¼ˆå¦‚ RTX 30/40ç³»åˆ—ï¼ŒA100/H100ï¼‰ï¼Œè¿™é‡Œæ¨èç”¨ torch.bfloat16ï¼Œå®ƒçš„æ•°å€¼ç¨³å®šæ€§æ¯” float16 æ›´å¥½ï¼Œè®­ç»ƒä¸å®¹æ˜“å‘æ•£ã€‚\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c452584b",
   "metadata": {},
   "source": [
    "# é»‘ç›’è’¸é¦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba38d04",
   "metadata": {},
   "source": [
    "## æµ‹è¯•Transformer ä¸­TraniningArgumentså‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0219f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. åŸºç¡€ç¯å¢ƒæ£€æŸ¥:\n",
      "   PyTorchç‰ˆæœ¬: 2.9.1+cu130\n",
      "   CUDAå¯ç”¨: True\n",
      "   GPUæ•°é‡: 1\n",
      "\n",
      "2. å°è¯•åˆ›å»ºTrainingArguments...\n",
      "   âŒ å¤±è´¥: Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# åº”ç”¨æ–¹æ³•ä¸€çš„æ ¸å¿ƒè®¾ç½®\n",
    "os.environ[\"ACCELERATE_DISABLE_MULTIPROCESSING\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # æˆ–è€… \"\" ç”¨äºCPU\n",
    "\n",
    "import torch\n",
    "print(\"1. åŸºç¡€ç¯å¢ƒæ£€æŸ¥:\")\n",
    "print(f\"   PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"   CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "print(f\"   GPUæ•°é‡: {torch.cuda.device_count()}\")\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "print(\"\\n2. å°è¯•åˆ›å»ºTrainingArguments...\")\n",
    "try:\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"./test\",\n",
    "        per_device_train_batch_size=2,\n",
    "        num_train_epochs=1,\n",
    "        report_to=\"none\",\n",
    "        no_cuda=False,  # æ ¹æ®ä½ çš„éœ€æ±‚è°ƒæ•´\n",
    "        use_cpu=False,\n",
    "    )\n",
    "    print(\"   âœ… æˆåŠŸï¼\")\n",
    "    print(f\"   è®¾å¤‡: {args.device}\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e99f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import torch\n",
    "\n",
    "# # 1. å¼ºåˆ¶è®¾ç½®å•è¿›ç¨‹ç¯å¢ƒå˜é‡ (ä¿æŒè®¾ç½®)\n",
    "# os.environ[\"ACCELERATE_DISABLE_MULTIPROCESSING\"] = \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # ä½¿ç”¨ç¬¬ä¸€å—GPUã€‚å¦‚æœåªç”¨CPUï¼Œè¯·æ”¹ä¸º os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "# os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "# os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "# os.environ[\"RANK\"] = \"0\"\n",
    "# os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "\n",
    "# # 2. æ ¸å¿ƒï¼šåœ¨å¯¼å…¥ä»»ä½• transformers æ¨¡å—å‰ï¼Œå…ˆä¿®è¡¥å…¶å†…éƒ¨ç±»\n",
    "# # æˆ‘ä»¬ç›´æ¥ä¿®æ”¹ TrainingArguments ç±»çš„ _setup_devices æ–¹æ³•ï¼Œè®©å®ƒç›´æ¥è¿”å›ä¸€ä¸ªç®€å•çš„å•è®¾å¤‡çŠ¶æ€ã€‚\n",
    "\n",
    "# # é¦–å…ˆï¼Œå¯¼å…¥éœ€è¦ä¿®æ”¹çš„æ¨¡å—\n",
    "# import transformers.training_args\n",
    "\n",
    "# # ä¿å­˜åŸå§‹æ–¹æ³•ï¼ˆå¯é€‰ï¼Œç”¨äºæ¢å¤ï¼‰\n",
    "# _original_setup_devices = transformers.training_args.TrainingArguments._setup_devices\n",
    "\n",
    "# # å®šä¹‰æˆ‘ä»¬çš„æ–°æ–¹æ³•ï¼Œå®ƒç›´æ¥æ„é€ ä¸€ä¸ªç®€å•çš„çŠ¶æ€å¯¹è±¡ï¼Œå®Œå…¨ç»•è¿‡ PartialState\n",
    "# def _patched_setup_devices(self):\n",
    "#     \"\"\"\n",
    "#     ä¸€ä¸ªä¼ªé€ çš„ _setup_devices æ–¹æ³•ã€‚\n",
    "#     å®ƒè¿”å›ä¸€ä¸ªå…·æœ‰å¿…è¦å±æ€§çš„ç®€å•å¯¹è±¡ï¼Œæ¨¡æ‹Ÿå•æœºå•å¡/CPUçš„çŠ¶æ€ã€‚\n",
    "#     \"\"\"\n",
    "#     class _FakeState:\n",
    "#         @property\n",
    "#         def device(self):\n",
    "#             # æ ¹æ®å‚æ•°å†³å®šè®¾å¤‡\n",
    "#             if hasattr(self, '_args') and getattr(self._args, 'no_cuda', False):\n",
    "#                 return torch.device(\"cpu\")\n",
    "#             if torch.cuda.is_available():\n",
    "#                 return torch.device(\"cuda:0\")\n",
    "#             return torch.device(\"cpu\")\n",
    "        \n",
    "#         is_local_main_process = True\n",
    "#         is_main_process = True\n",
    "#         process_index = 0\n",
    "#         num_processes = 1\n",
    "#         local_process_index = 0\n",
    "#         _distributed_type = \"NO\"  # æ¨¡æ‹Ÿéåˆ†å¸ƒå¼çŠ¶æ€\n",
    "    \n",
    "#     # åˆ›å»ºä¸€ä¸ªå‡çŠ¶æ€å¯¹è±¡ï¼Œå¹¶æŠŠå½“å‰ TrainingArguments å®ä¾‹ä¼ ç»™å®ƒä»¥å¤‡æŸ¥è¯¢\n",
    "#     fake_state = _FakeState()\n",
    "#     fake_state._args = self  # è®©å‡å¯¹è±¡èƒ½è®¿é—®åˆ° no_cuda ç­‰å‚æ•°\n",
    "    \n",
    "#     # ç›´æ¥è¿”å›è¿™ä¸ªå‡å¯¹è±¡ï¼Œé˜»æ­¢åç»­æ‰€æœ‰å¤æ‚çš„åˆå§‹åŒ–\n",
    "#     return fake_state\n",
    "\n",
    "# # åº”ç”¨è¡¥ä¸ï¼šç”¨æˆ‘ä»¬çš„æ–¹æ³•æ›¿æ¢æ‰åŸæ–¹æ³•\n",
    "# transformers.training_args.TrainingArguments._setup_devices = _patched_setup_devices\n",
    "\n",
    "# print(\"âœ… å·²æˆåŠŸåº”ç”¨è¡¥ä¸ï¼Œå¼ºåˆ¶ TrainingArguments ä½¿ç”¨å•è®¾å¤‡æ¨¡å¼ã€‚\")\n",
    "# # --- ä¿®è¡¥ä»£ç ç»“æŸ ---\n",
    "\n",
    "# # 3. ç°åœ¨å¯ä»¥å®‰å…¨åœ°å¯¼å…¥ transformers å¹¶åˆ›å»º TrainingArguments\n",
    "# from transformers import TrainingArguments\n",
    "# print(\"å¼€å§‹åˆ›å»º TrainingArguments...\")\n",
    "\n",
    "# # æµ‹è¯•åˆ›å»º\n",
    "# args = TrainingArguments(\n",
    "#     output_dir=\"./test_output\",\n",
    "#     per_device_train_batch_size=2,\n",
    "#     num_train_epochs=1,\n",
    "#     report_to=\"none\",\n",
    "#     no_cuda=True,  # å¼ºåˆ¶CPUã€‚å¦‚æœä½ æƒ³ç”¨GPUä¸”æœ‰CUDAï¼Œè¯·è®¾ç½®ä¸º False\n",
    "#     use_cpu=True,  # å†æ¬¡ç¡®ä¿\n",
    "# )\n",
    "\n",
    "# print(f\"âœ… åˆ›å»ºæˆåŠŸï¼\")\n",
    "# print(f\"   é¢„æœŸè¿è¡Œè®¾å¤‡: CPU\")\n",
    "# # ç”±äºæˆ‘ä»¬æ‰“äº†è¡¥ä¸ï¼Œç›´æ¥è®¿é—® args.device å¯èƒ½ä¸ä¼šè§¦å‘é”™è¯¯ï¼Œä½†å†…éƒ¨æœºåˆ¶å·²æ­£å¸¸å·¥ä½œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58769290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed86d304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch ç‰ˆæœ¬: 2.9.1+cu130\n",
      "CUDA æ˜¯å¦å¯ç”¨: True\n",
      "GPU æ•°é‡: 1\n",
      "å½“å‰GPU: 0\n",
      "GPU åç§°: NVIDIA GeForce RTX 5080\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 124\u001b[0m\n\u001b[0;32m     78\u001b[0m collator \u001b[38;5;241m=\u001b[39m DataCollatorForCompletionOnlyLM(response_template, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# 6. é…ç½®è®­ç»ƒå‚æ•°\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# ä½œç”¨ï¼ŒTrainingArguments æ˜¯ Hugging Face transformers åº“ä¸­ç”¨äºå®šä¹‰è®­ç»ƒè¿‡ç¨‹æ‰€æœ‰è¶…å‚æ•°ï¼ˆHyperparametersï¼‰çš„æ ¸å¿ƒç±»ã€‚\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# å®ƒè´Ÿè´£å‘Šè¯‰è®­ç»ƒå™¨ï¼ˆTrainerï¼‰â€œè¯¥æ€ä¹ˆç»ƒâ€â€”â€”æ¯”å¦‚ç»ƒå¤šå¿«ï¼ˆå­¦ä¹ ç‡ï¼‰ã€ç»ƒå¤šå°‘æ¬¡ï¼ˆEpochï¼‰ã€å å¤šå°‘æ˜¾å­˜ï¼ˆBatch Sizeï¼‰ã€å¾€å“ªé‡Œå­˜ï¼ˆOutput Dirï¼‰ç­‰ã€‚\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# noneï¼Œ è¡¨ç¤ºå…³é—­ä¸Šä¼ åŠŸèƒ½ã€‚\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# åœ¨æœ¬åœ°å¼€å‘ã€æµ‹è¯•æˆ–è€…ä¸æƒ³æ³¨å†ŒWandBè´¦å·æ—¶ï¼Œè®¾ç½®ä¸ºnoneå¯ä»¥é¿å…å¾ˆå¤šç™»å½•æŠ¥é”™å’Œç½‘ç»œè¿æ¥é—®é¢˜\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# o_cuda=False,  # å¼ºåˆ¶ä½¿ç”¨CPU\u001b[39;49;00m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# æ·»åŠ ä»¥ä¸‹ä¸¤è¡Œå…³é”®å‚æ•°\u001b[39;49;00m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# local_rank=-1,          # æ˜ç¡®æŒ‡å®šä¸ºéåˆ†å¸ƒå¼æ¨¡å¼\u001b[39;49;00m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ddp_find_unused_parameters=False, # å…³é—­åˆ†å¸ƒå¼ä¸‹å¯»æ‰¾æœªä½¿ç”¨å‚æ•°çš„é€‰é¡¹ï¼ˆå•å¡æ—¶æ— æ•ˆï¼Œä½†å¯é¿å…è­¦å‘Šï¼‰\u001b[39;49;00m\n\u001b[0;32m    137\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# ====== æ·»åŠ /ä¿®æ”¹ä»¥ä¸‹å‚æ•° ======\u001b[39;49;00m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# local_rank=-1,                   # å¼ºåˆ¶æŒ‡å®šå½“å‰è¿›ç¨‹çš„æœ¬åœ°æ’åä¸º -1ï¼ˆéåˆ†å¸ƒå¼ï¼‰\u001b[39;49;00m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ddp_backend=None,                # æ˜¾å¼æŒ‡å®šåˆ†å¸ƒå¼åç«¯ä¸º None\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ddp_find_unused_parameters=False, # å…³é—­åˆ†å¸ƒå¼è®­ç»ƒä¸­å¯»æ‰¾æœªä½¿ç”¨å‚æ•°çš„é€‰é¡¹ï¼ˆå•å¡æ—¶æ— æ•ˆï¼Œä½†å¯é¿å…è­¦å‘Šï¼‰\u001b[39;49;00m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# å¦‚æœç¡®å®šä½¿ç”¨CPUï¼Œå¯é¢å¤–æ·»åŠ ï¼š\u001b[39;49;00m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# no_cuda=True,                   # å¼ºåˆ¶ä½¿ç”¨CPU\u001b[39;49;00m\n\u001b[0;32m    143\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# 7. å¼€å§‹è®­ç»ƒ\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# SFTTrainerï¼ˆSupervised Fine-Tuning Trainerï¼‰æ˜¯ trl åº“æä¾›çš„æ ¸å¿ƒç±»ã€‚\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# å®ƒçš„ä½œç”¨æ˜¯å°†ä¹‹å‰å‡†å¤‡å¥½çš„æ¨¡å‹ã€æ•°æ®ã€è¶…å‚æ•°ã€å¾®è°ƒç­–ç•¥å…¨éƒ¨ç»„è£…åœ¨ä¸€èµ·ï¼Œæ„å»ºä¸€ä¸ªå¯ä»¥æ‰§è¡Œè®­ç»ƒä»»åŠ¡çš„å¯¹è±¡ã€‚\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# ç›¸æ¯”äº Hugging Face åŸç”Ÿçš„ Trainerï¼ŒSFTTrainer åšäº†å¾ˆå¤šé’ˆå¯¹æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰çš„è‡ªåŠ¨åŒ–å·¥ä½œï¼ˆå¦‚è‡ªåŠ¨åˆ†è¯ã€è‡ªåŠ¨åº”ç”¨ LoRAã€è‡ªåŠ¨å¤„ç† Dataset æ ¼å¼ç­‰ï¼‰ã€‚\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸš€ å¼€å§‹é»‘ç›’è’¸é¦è®­ç»ƒ (SFT)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m<string>:135\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, project, trackio_space_id, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\transformers\\training_args.py:1811\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1809\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[0;32m   1810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[1;32m-> 1811\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorchdynamo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1814\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torchdynamo` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_compile_backend` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1817\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\transformers\\training_args.py:2355\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2352\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   2353\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2354\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 2355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\functools.py:981\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    979\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m--> 981\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    983\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\transformers\\training_args.py:2282\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_deepspeed:\n\u001b[0;32m   2281\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mACCELERATE_USE_DEEPSPEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_state \u001b[38;5;241m=\u001b[39m PartialState(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maccelerator_state_kwargs)\n\u001b[0;32m   2283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_deepspeed:\n\u001b[0;32m   2284\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mACCELERATE_USE_DEEPSPEED\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\accelerate\\state.py:236\u001b[0m, in \u001b[0;36mPartialState.__init__\u001b[1;34m(self, cpu, **kwargs)\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnccl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mACCELERATE_USE_FSDP\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m                 )\n\u001b[0;32m    234\u001b[0m             ):\n\u001b[0;32m    235\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:nccl,cpu:gloo\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 236\u001b[0m             torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39minit_process_group(backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# XPU and CPU require special env configs to be set\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;129;01min\u001b[39;00m (DistributedType\u001b[38;5;241m.\u001b[39mMULTI_XPU, DistributedType\u001b[38;5;241m.\u001b[39mMULTI_CPU):\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\torch\\distributed\\c10d_logger.py:81\u001b[0m, in \u001b[0;36m_exception_logger.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m     83\u001b[0m         msg_dict \u001b[38;5;241m=\u001b[39m _get_msg_dict(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\torch\\distributed\\c10d_logger.py:95\u001b[0m, in \u001b[0;36m_time_logger.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _WaitCounter(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch.wait_counter.c10d.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mguard():\n\u001b[1;32m---> 95\u001b[0m         func_return \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func_return\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\torch\\distributed\\distributed_c10d.py:1762\u001b[0m, in \u001b[0;36minit_process_group\u001b[1;34m(backend, init_method, timeout, world_size, rank, store, group_name, pg_options, device_id)\u001b[0m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1759\u001b[0m     rendezvous_iterator \u001b[38;5;241m=\u001b[39m rendezvous(\n\u001b[0;32m   1760\u001b[0m         not_none(init_method), rank, world_size, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m   1761\u001b[0m     )\n\u001b[1;32m-> 1762\u001b[0m     store, rank, world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrendezvous_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     store\u001b[38;5;241m.\u001b[39mset_timeout(timeout)\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# Use a PrefixStore to avoid accidental overrides of keys used by\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# different systems (e.g. RPC) in case the store is multi-tenant.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\torch\\distributed\\rendezvous.py:267\u001b[0m, in \u001b[0;36m_env_rendezvous_handler\u001b[1;34m(url, timeout, **kwargs)\u001b[0m\n\u001b[0;32m    265\u001b[0m     rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(query_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 267\u001b[0m     rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43m_get_env_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRANK\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworld_size\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m query_dict:\n\u001b[0;32m    270\u001b[0m     world_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(query_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworld_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32md:\\Anaconda\\deeplearn\\lib\\site-packages\\torch\\distributed\\rendezvous.py:252\u001b[0m, in \u001b[0;36m_env_rendezvous_handler.<locals>._get_env_or_raise\u001b[1;34m(env_var)\u001b[0m\n\u001b[0;32m    250\u001b[0m env_val \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(env_var, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_val:\n\u001b[1;32m--> 252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _env_error(env_var)\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_val\n",
      "\u001b[1;31mValueError\u001b[0m: Error initializing torch.distributed using env:// rendezvous: environment variable RANK expected, but not set"
     ]
    }
   ],
   "source": [
    "import os;\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # æŒ‡å®šä½¿ç”¨ç¬¬ä¸€å—GPUï¼Œå¦‚æœåªæœ‰ä¸€å—ï¼Œè¿™å°±æ˜¯ \"0\"\n",
    "import torch\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDA æ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU æ•°é‡: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"å½“å‰GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU åç§°: {torch.cuda.get_device_name()}\")\n",
    "# 3. è®¾ç½® LoRA (å¯é€‰ï¼Œä½†æ¨è)\n",
    "# å…¨å‚æ•°å¾®è°ƒ0.5Bä¹Ÿå¯ä»¥ï¼Œä½†LoRAæ›´å¿«æ›´ç¨³\n",
    "# LoRAä¸æ›´æ–°æ¨¡å‹åŸæœ¬çš„å‡ åäº¿å‚æ•°ï¼Œè€Œæ˜¯ç»™æ¨¡å‹ä¸­æŒ‡å®šçš„å±‚æ’å…¥ä¸¤ä¸ªä½ç§©å°çŸ©é˜µæ¥è®­ç»ƒã€‚è¿™æ ·æ˜¾å­˜å ç”¨æä½ï¼Œä¸”æ•ˆæœé€¼è¿‘å…¨é‡å¾®è°ƒã€‚\n",
    "# r=8ï¼Œç§© (Rank)ã€‚\n",
    "#   è¿™æ˜¯ LoRA ä¸­æœ€é‡è¦çš„å‚æ•°ã€‚å®ƒå†³å®šäº†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚æ•°å€¼è¶Šå¤§ï¼Œæ¨¡å‹èƒ½å­¦åˆ°çš„ä¿¡æ¯è¶Šå¤šï¼Œä½†æ˜¾å­˜å ç”¨å’Œè®¡ç®—é‡ä¹Ÿè¶Šå¤§ã€‚\n",
    "#   é€šå¸¸è®¾ä¸º 8, 16, 32 æˆ– 64ã€‚å¯¹äºä¸€èˆ¬å¾®è°ƒï¼Œ8 æˆ– 16 æ˜¯æ€§ä»·æ¯”æœ€é«˜çš„é€‰æ‹©ã€‚\n",
    "# lora_alpha=16ï¼Œç¼©æ”¾ç³»æ•° (Scaling Factor)ã€‚\n",
    "#   ç±»ä¼¼äºå­¦ä¹ ç‡æ”¾å¤§å™¨ã€‚LoRA çš„æƒé‡æ›´æ–°ä¼šè¢«ä¹˜ä»¥ alpha/rã€‚\n",
    "#   é€šå¸¸è®¾ç½®ä¸º r çš„ 2å€ï¼ˆå³ alpha=2rï¼‰æ˜¯ç»éªŒä¸Šçš„æœ€ä½³å®è·µã€‚\n",
    "# target_modules=[...]ï¼Œç›®æ ‡æ¨¡å—ã€‚æŒ‡å®šè¦å¯¹æ¨¡å‹çš„å“ªäº›å±‚åº”ç”¨ LoRAã€‚\n",
    "#   åˆ—è¡¨ä¸­çš„åå­—ï¼ˆå¦‚ q_projï¼‰å¯¹åº” Transformer å†…éƒ¨çš„çº¿æ€§å±‚åç§°ã€‚\n",
    "#   ç‰¹åˆ«è¯´æ˜ï¼Œå¯¹äº Qwenã€Llama ç­‰æ¨¡å‹ï¼Œå®˜æ–¹å»ºè®®å¯¹æ‰€æœ‰çº¿æ€§å±‚ï¼ˆAttention å±‚çš„ Q,K,V,O å’Œ MLP å±‚çš„ Gate,Up,Downï¼‰éƒ½è¿›è¡Œå¾®è°ƒï¼Œæ•ˆæœè¿œå¥½äºåªå¾®è°ƒ Attention å±‚ã€‚\n",
    "# lora_dropout=0.05ï¼ŒDropout ç‡ã€‚\n",
    "#   åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºä¸¢å¼ƒ 5% çš„ç¥ç»å…ƒè¿æ¥ï¼Œé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆï¼ˆæ­»è®°ç¡¬èƒŒè®­ç»ƒæ•°æ®ï¼‰ã€‚\n",
    "# bias=\"none\"ï¼Œåç½®é¡¹è®¾ç½®ã€‚\n",
    "#   \"none\" è¡¨ç¤ºä¸è®­ç»ƒåŸæ¨¡å‹ä¸­çš„ Bias å‚æ•°ã€‚è¿™æ˜¯ä¸ºäº†æœ€å¤§ç¨‹åº¦èŠ‚çœæ˜¾å­˜ã€‚ä¹Ÿå¯ä»¥é€‰ \"all\" æˆ– \"lora_only\"ï¼Œä½†é€šå¸¸ \"none\" å°±å¤Ÿäº†ã€‚\n",
    "# task_type=\"CAUSAL_LM\"ï¼Œä»»åŠ¡ç±»å‹ã€‚\n",
    "#   å‘Šè¯‰ LoRA æˆ‘ä»¬åœ¨åšä¸€ä¸ªæ ‡å‡†çš„â€œå› æœè¯­è¨€æ¨¡å‹â€ä»»åŠ¡ï¼ˆå³ GPT å¼çš„æ–‡æœ¬ç”Ÿæˆï¼‰ã€‚\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # Qwen å…¨çº¿æ€§å±‚å¾®è°ƒæ•ˆæœæœ€å¥½\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# 4. æ ¼å¼åŒ–å‡½æ•°ã€‚è¿™æ˜¯ä¸€ä¸ªå›è°ƒå‡½æ•°ï¼Œä¼šè¢« SFTTrainer è‡ªåŠ¨è°ƒç”¨ã€‚\n",
    "# å®ƒçš„ç›®çš„æ˜¯æŠŠæ•°æ®é›†ä¸­çš„åŸå§‹åˆ—ï¼ˆinstruction, thinking, outputï¼‰è½¬æ¢æˆæ¨¡å‹èƒ½çœ‹æ‡‚çš„ä¸€ä¸ªé•¿å­—ç¬¦ä¸²ã€‚\n",
    "\n",
    "# æ€ç»´é“¾èåˆï¼Œä»£ç å°† thinkingï¼ˆæ€ç»´é“¾/CoTï¼‰å’Œ outputï¼ˆæ ‡å‡†ç­”æ¡ˆï¼‰æ‹¼åœ¨äº†ä¸€èµ·ã€‚\n",
    "# è¿™æ„å‘³ç€æˆ‘ä»¬åœ¨æ•™æ¨¡å‹åœ¨å›ç­”é—®é¢˜å‰å…ˆè¾“å‡ºæ€è€ƒè¿‡ç¨‹ã€‚è¿™æ˜¯ç›®å‰ DeepSeek-R1 ç­‰æ¨ç†æ¨¡å‹çš„æ ¸å¿ƒè®­ç»ƒæ–¹å¼ã€‚\n",
    "\n",
    "# example å‚æ•°ï¼šè¿™æ˜¯ä¼ å…¥çš„ä¸€æ‰¹æ•°æ®ï¼ˆBatchï¼‰ï¼ŒåŒ…å«å¤šæ¡æ ·æœ¬ã€‚\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        # æ‹¼æ¥ \"æ€è€ƒè¿‡ç¨‹\" å’Œ \"æœ€ç»ˆç­”æ¡ˆ\"\n",
    "        output = \"[æ€è€ƒè¿‡ç¨‹]\\n\"+example[\"thinking\"][i]+\"\\n\\n[æœ€ç»ˆç­”æ¡ˆ]\\n\"+example[\"output\"][i] # è¿™é‡ŒåŒ…å«äº† Teacher ç”Ÿæˆçš„æ€ç»´é“¾\n",
    "        # æ„å»º Qwen æ ¼å¼æ–‡æœ¬\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": example['instruction'][i]},\n",
    "            {\"role\": \"assistant\", \"content\": output} # è¿™é‡Œçš„ output åŒ…å«æ€ç»´é“¾\n",
    "        ]\n",
    "        # åº”ç”¨åˆ†è¯å™¨çš„èŠå¤©æ¨¡æ¿\n",
    "        # æ ¹æ® Qwen æ¨¡å‹çš„è¦æ±‚ï¼Œè‡ªåŠ¨æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼ˆSpecial Tokensï¼‰ã€‚\n",
    "        #   ä¾‹å¦‚ï¼šå®ƒä¼šæŠŠ messages å˜æˆç±»ä¼¼è¿™æ ·çš„å­—ç¬¦ä¸²ï¼š\n",
    "        #   <|im_start|>system\\nYou are...<|im_end|><|im_start|>user\\né—®é¢˜...<|im_end|><|im_start|>assistant\\n[æ€è€ƒè¿‡ç¨‹]...\n",
    "        #   tokenize=Falseï¼šéå¸¸å…³é”®ã€‚æˆ‘ä»¬è¿™é‡Œåªç”Ÿæˆå­—ç¬¦ä¸²ï¼Œä¸è½¬æˆæ•°å­— IDã€‚å› ä¸º SFTTrainer å†…éƒ¨ä¼šç»Ÿä¸€åš Tokenizationã€‚\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "# 5. å…³é”®ï¼Œè¿™æ˜¯æŒ‡ä»¤å¾®è°ƒï¼ˆSFTï¼‰ä¸­æœ€é‡è¦çš„ä¸€ä¸ªç»„ä»¶ã€‚å®ƒçš„ä½œç”¨æ˜¯Maskï¼ˆæ©ç›–ï¼‰æ‰ç”¨æˆ·æé—®éƒ¨åˆ†çš„ Lossã€‚\n",
    "# è¿™ä¸ªç¥å™¨ä¼šè‡ªåŠ¨å¸®ä½ æŠŠ User çš„éƒ¨åˆ† mask æ‰ (è®¾ä¸º -100)ï¼Œåªè®¡ç®— Assistant å›å¤çš„ Loss\n",
    "\n",
    "# ä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ\n",
    "#   å¦‚æœä½ ä¸åŠ è¿™ä¸ªï¼Œæ¨¡å‹åœ¨è®­ç»ƒæ—¶ä¼šåŒæ—¶è®¡ç®—â€œç”¨æˆ·é—®é¢˜â€å’Œâ€œAIå›ç­”â€çš„æŸå¤±ã€‚\n",
    "#   è¿™ä¼šå¯¼è‡´æ¨¡å‹é€šè¿‡èƒŒè¯µç”¨æˆ·çš„é—®é¢˜æ¥é™ä½ Lossï¼Œè€Œä¸æ˜¯å­¦ä¹ å¦‚ä½•å›ç­”ã€‚\n",
    "#   ä½¿ç”¨äº†è¿™ä¸ª Collator åï¼Œæ¨¡å‹åªä¼šè¢«æƒ©ç½šå›ç­”å¾—ä¸å¯¹çš„åœ°æ–¹ï¼Œè€Œä¸ä¼šè¢«æƒ©ç½šå¤è¿°é—®é¢˜çš„åœ°æ–¹ã€‚\n",
    "\n",
    "# å‚æ•°è¯¦ç»†è¯´æ˜ï¼š\n",
    "#   response_templateï¼Œåˆ†éš”ç¬¦ã€‚å‘Šè¯‰ Collator ä»å“ªé‡Œå¼€å§‹æ˜¯â€œAI çš„å›ç­”â€ã€‚\n",
    "#   <|im_start|>assistant\\nï¼šè¿™æ˜¯ Qwen æ¨¡å‹ç‰¹æœ‰çš„ç‰¹æ®Š Token åºåˆ—ï¼Œæ ‡å¿—ç€ Assistant å¼€å§‹è¯´è¯ã€‚\n",
    "#   Collator ä¼šåœ¨æ•´æ®µæ–‡æœ¬ä¸­å¯»æ‰¾è¿™ä¸ªå­—ç¬¦ä¸²ï¼ŒæŠŠè¿™ä¸ªå­—ç¬¦ä¸²ä¹‹å‰çš„æ‰€æœ‰ Tokenï¼ˆSystem Prompt + User Promptï¼‰çš„ Label è®¾ä¸º -100ï¼ˆPyTorch ä¸­å¿½ç•¥è®¡ç®— Loss çš„æ ‡è®°ï¼‰ã€‚\n",
    "# tokenizer=tokenizerï¼ŒCollator éœ€è¦åˆ†è¯å™¨æ¥æŠŠ response_template å­—ç¬¦ä¸²è½¬æˆ Token IDï¼Œä»¥ä¾¿åœ¨è½¬æ¢åçš„æ•°æ®ä¸­è¿›è¡ŒåŒ¹é…æŸ¥æ‰¾ã€‚\n",
    "response_template = \"<|im_start|>assistant\\n\" \n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "# 6. é…ç½®è®­ç»ƒå‚æ•°\n",
    "# ä½œç”¨ï¼ŒTrainingArguments æ˜¯ Hugging Face transformers åº“ä¸­ç”¨äºå®šä¹‰è®­ç»ƒè¿‡ç¨‹æ‰€æœ‰è¶…å‚æ•°ï¼ˆHyperparametersï¼‰çš„æ ¸å¿ƒç±»ã€‚\n",
    "# å®ƒè´Ÿè´£å‘Šè¯‰è®­ç»ƒå™¨ï¼ˆTrainerï¼‰â€œè¯¥æ€ä¹ˆç»ƒâ€â€”â€”æ¯”å¦‚ç»ƒå¤šå¿«ï¼ˆå­¦ä¹ ç‡ï¼‰ã€ç»ƒå¤šå°‘æ¬¡ï¼ˆEpochï¼‰ã€å å¤šå°‘æ˜¾å­˜ï¼ˆBatch Sizeï¼‰ã€å¾€å“ªé‡Œå­˜ï¼ˆOutput Dirï¼‰ç­‰ã€‚\n",
    "\n",
    "# output_dirï¼Œ æŒ‡å®šè¾“å‡ºç›®å½•ã€‚\n",
    "#   è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆçš„æ‰€æœ‰æ–‡ä»¶éƒ½ä¼šå­˜æ”¾åœ¨è¿™é‡Œã€‚åŒ…æ‹¬ï¼š\n",
    "#       Checkpointsï¼ˆæ£€æŸ¥ç‚¹ï¼‰ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¨¡å‹æƒé‡çš„ä¿å­˜ã€‚\n",
    "#       Logsï¼ˆæ—¥å¿—ï¼‰ï¼šLoss çš„å˜åŒ–è®°å½•ç­‰ã€‚\n",
    "#       Configï¼ˆé…ç½®ï¼‰ï¼šæœ€ç»ˆæ¨¡å‹çš„é…ç½®æ–‡ä»¶ã€‚\n",
    "\n",
    "# per_device_train_batch_sizeï¼Œå•å¼ æ˜¾å¡ä¸Šçš„è®­ç»ƒæ‰¹æ¬¡å¤§å°ã€‚\n",
    "#   å®ƒå†³å®šäº†æ¯æ¬¡å‚æ•°æ›´æ–°å‰ï¼Œæ˜¾å¡ä¸€æ¬¡æ€§è¯»å…¥å¤šå°‘æ¡æ•°æ®è¿›è¡Œè®¡ç®—ã€‚\n",
    "#   æ•°å€¼å«ä¹‰ï¼šè¿™é‡Œè®¾ä¸º 8ï¼Œæ„å‘³ç€å¦‚æœä½ çš„æ˜¾å­˜å¤Ÿå¤§ï¼Œæ¯æ¬¡ä¼šåŒæ—¶æŠŠ 8 æ¡æ•°æ®å¡è¿›æ˜¾å¡è®¡ç®— Lossã€‚\n",
    "#   æ˜¾å­˜å½±å“ï¼šè¿™ä¸ªæ•°å€¼è¶Šå¤§ï¼Œè®­ç»ƒè¶Šå¿«ï¼Œä½†æ˜¾å­˜å ç”¨è¶Šé«˜ã€‚å¦‚æœæŠ¥ OOMï¼ˆæ˜¾å­˜æº¢å‡ºï¼‰é”™è¯¯ï¼Œé€šå¸¸ç¬¬ä¸€ä¸ªè¦æ”¹å°çš„å°±æ˜¯è¿™ä¸ªå‚æ•°ã€‚\n",
    "\n",
    "# gradient_accumulation_stepsï¼Œæ¢¯åº¦ç´¯ç§¯æ­¥æ•°ã€‚\n",
    "#   è¿™æ˜¯å°æ˜¾å­˜è·‘å¤§ Batch Size çš„æ ¸å¿ƒæŠ€å·§ã€‚\n",
    "#   åŸç†ï¼šæ¨¡å‹ä¼šè¿›è¡Œ 4 æ¬¡å‰å‘ä¼ æ’­ï¼ˆForward Passï¼‰ï¼Œæ¯æ¬¡ç®—å®Œ Loss åä¸ç«‹å³æ›´æ–°æƒé‡ï¼Œè€Œæ˜¯æŠŠæ¢¯åº¦æ”’èµ·æ¥ã€‚ç­‰å‡‘å¤Ÿäº† 4 æ¬¡ï¼Œå†ä¸€æ¬¡æ€§æ›´æ–°æƒé‡ã€‚\n",
    "#   å®é™…/ç­‰æ•ˆ Batch Sizeï¼šTotal Batch Size=per_device_batch_sizeÃ—gradient_accumulation_stepsÃ—GPUæ•°é‡\n",
    "#   åœ¨æœ¬ä¾‹ä¸­ï¼š8Ã—4Ã—1=32\n",
    "#   è¿™æ„å‘³ç€è™½ç„¶å•æ¬¡åªè¯» 8 æ¡æ•°æ®ï¼Œä½†åœ¨æ•°å­¦ä¸Šç­‰åŒäºä¸€æ¬¡è¯»äº† 32 æ¡æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»è€Œä¿è¯äº†è®­ç»ƒçš„ç¨³å®šæ€§ã€‚\n",
    "\n",
    "# learning_rateï¼Œ å­¦ä¹ ç‡ï¼ˆOptimizer çš„æ­¥é•¿ï¼‰ã€‚\n",
    "# å†³å®šäº†æ¨¡å‹å‚æ•°æ¯æ¬¡æ›´æ–°å˜åŒ–çš„å¹…åº¦ã€‚\n",
    "# æ•°å€¼èƒŒæ™¯ï¼š2e-4ï¼ˆå³ 0.0002ï¼‰æ˜¯ LoRA å¾®è°ƒçš„ç»éªŒå€¼ã€‚\n",
    "# å¯¹æ¯”ï¼šå¦‚æœæ˜¯å…¨é‡å¾®è°ƒï¼ˆFull Fine-tuningï¼‰ï¼Œå­¦ä¹ ç‡é€šå¸¸å¾ˆå°ï¼ˆå¦‚ 1e-5 æˆ– 2e-5ï¼‰ã€‚ä½†å› ä¸º LoRA è®­ç»ƒå‚æ•°å¾ˆå°‘ï¼Œé€šå¸¸éœ€è¦æ›´å¤§çš„å­¦ä¹ ç‡æ‰èƒ½æ”¶æ•›ã€‚\n",
    "\n",
    "# logging_stepsï¼Œæ—¥å¿—æ‰“å°é¢‘ç‡ã€‚\n",
    "# è®¾ç½®æ¯è®­ç»ƒå¤šå°‘æ­¥ï¼ˆStepï¼‰å°±åœ¨æ§åˆ¶å°æ‰“å°ä¸€æ¬¡å½“å‰çš„ Loss å’Œå­¦ä¹ ç‡ã€‚\n",
    "# è®¾ä¸º 1 æ˜¯å› ä¸ºæ•°æ®é›†æå°åœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­ï¼Œé€šå¸¸è®¾ç½®ä¸º 10 æˆ– 100ï¼Œå¦åˆ™å±å¹•ä¼šè¢«æ—¥å¿—åˆ·å±ï¼Œä¸”è½»å¾®å½±å“æ€§èƒ½ã€‚\n",
    "\n",
    "# num_train_epochs=3ï¼Œè®­ç»ƒè½®æ¬¡ï¼Œä¸€ä¸ªepochæ„å‘³ç€æ¨¡å‹æŠŠæ•´ä¸ªè®­ç»ƒé›†ä»å¤´åˆ°å°¾è·‘ä¸€é\n",
    "# è®¾ç½®ä¸º3æ˜¯å¾®è°ƒé¢†åŸŸçš„â€œé»„é‡‘æ ‡å‡†â€ï¼Œé€šå¸¸1-3ä¸ªepochå°±èƒ½å–å¾—å¾ˆå¥½çš„æ•ˆæœã€‚å†å¤šç»ƒå¯èƒ½ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå¤±å»æ³›åèƒ½åŠ›\n",
    "\n",
    "# save_strategy=\"epoch\", æ¨¡å‹ä¿å­˜çš„ç­–ç•¥ã€‚\n",
    "# å†³å®šäº†ä»€ä¹ˆæ—¶å€™ä¿å­˜ä¸€ä¸ªCheckjpoint(å­˜æ¡£)\n",
    "# epoch, è¡¨ç¤ºæ¯è·‘å®Œä¸€è½®ï¼Œå°±è‡ªåŠ¨ä¿å­˜ä¸€æ¬¡æ¨¡å‹\n",
    "# stepsï¼Œ æ¯éš”Næ­¥ä¿å­˜ä¸€æ¬¡ï¼ˆé…åˆsave_stepså‚æ•°ï¼‰\n",
    "# no, è®­ç»ƒç»“é€Ÿå‰ä¸ä¿å­˜ä»»ä½•ä¸­é—´ç»“æœï¼ˆçœç¡¬ç›˜ï¼‰\n",
    "\n",
    "# report_to=\"none\"ï¼Œå¯è§†åŒ–æŠ¥å‘Šå·¥å…·é›†æˆã€‚\n",
    "# transformeråº“æ”¯æŒè‡ªåŠ¨æŠŠè®­ç»ƒæ•°æ®ä¸Šä¼ åˆ°WandBï¼ˆWeights & Biases), TensorBoard, MLflowç­‰å¹³å°\n",
    "# noneï¼Œ è¡¨ç¤ºå…³é—­ä¸Šä¼ åŠŸèƒ½ã€‚\n",
    "# åœ¨æœ¬åœ°å¼€å‘ã€æµ‹è¯•æˆ–è€…ä¸æƒ³æ³¨å†ŒWandBè´¦å·æ—¶ï¼Œè®¾ç½®ä¸ºnoneå¯ä»¥é¿å…å¾ˆå¤šç™»å½•æŠ¥é”™å’Œç½‘ç»œè¿æ¥é—®é¢˜\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=8, \n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3, \n",
    "    save_strategy=\"epoch\", \n",
    "    report_to=\"none\",\n",
    "    # o_cuda=False,  # å¼ºåˆ¶ä½¿ç”¨CPU\n",
    "    # æ·»åŠ ä»¥ä¸‹ä¸¤è¡Œå…³é”®å‚æ•°\n",
    "    # local_rank=-1,          # æ˜ç¡®æŒ‡å®šä¸ºéåˆ†å¸ƒå¼æ¨¡å¼\n",
    "    # ddp_find_unused_parameters=False, # å…³é—­åˆ†å¸ƒå¼ä¸‹å¯»æ‰¾æœªä½¿ç”¨å‚æ•°çš„é€‰é¡¹ï¼ˆå•å¡æ—¶æ— æ•ˆï¼Œä½†å¯é¿å…è­¦å‘Šï¼‰\n",
    "      # ====== æ·»åŠ /ä¿®æ”¹ä»¥ä¸‹å‚æ•° ======\n",
    "    # local_rank=-1,                   # å¼ºåˆ¶æŒ‡å®šå½“å‰è¿›ç¨‹çš„æœ¬åœ°æ’åä¸º -1ï¼ˆéåˆ†å¸ƒå¼ï¼‰\n",
    "    # ddp_backend=None,                # æ˜¾å¼æŒ‡å®šåˆ†å¸ƒå¼åç«¯ä¸º None\n",
    "    # ddp_find_unused_parameters=False, # å…³é—­åˆ†å¸ƒå¼è®­ç»ƒä¸­å¯»æ‰¾æœªä½¿ç”¨å‚æ•°çš„é€‰é¡¹ï¼ˆå•å¡æ—¶æ— æ•ˆï¼Œä½†å¯é¿å…è­¦å‘Šï¼‰\n",
    "    # å¦‚æœç¡®å®šä½¿ç”¨CPUï¼Œå¯é¢å¤–æ·»åŠ ï¼š\n",
    "    # no_cuda=True,                   # å¼ºåˆ¶ä½¿ç”¨CPU\n",
    ")\n",
    "\n",
    "# 7. å¼€å§‹è®­ç»ƒ\n",
    "\n",
    "# SFTTrainerï¼ˆSupervised Fine-Tuning Trainerï¼‰æ˜¯ trl åº“æä¾›çš„æ ¸å¿ƒç±»ã€‚\n",
    "# å®ƒçš„ä½œç”¨æ˜¯å°†ä¹‹å‰å‡†å¤‡å¥½çš„æ¨¡å‹ã€æ•°æ®ã€è¶…å‚æ•°ã€å¾®è°ƒç­–ç•¥å…¨éƒ¨ç»„è£…åœ¨ä¸€èµ·ï¼Œæ„å»ºä¸€ä¸ªå¯ä»¥æ‰§è¡Œè®­ç»ƒä»»åŠ¡çš„å¯¹è±¡ã€‚\n",
    "# ç›¸æ¯”äº Hugging Face åŸç”Ÿçš„ Trainerï¼ŒSFTTrainer åšäº†å¾ˆå¤šé’ˆå¯¹æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰çš„è‡ªåŠ¨åŒ–å·¥ä½œï¼ˆå¦‚è‡ªåŠ¨åˆ†è¯ã€è‡ªåŠ¨åº”ç”¨ LoRAã€è‡ªåŠ¨å¤„ç† Dataset æ ¼å¼ç­‰ï¼‰ã€‚\n",
    "print(\"ğŸš€ å¼€å§‹é»‘ç›’è’¸é¦è®­ç»ƒ (SFT)...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 8. ä¿å­˜ä½ çš„æˆæœ\n",
    "print(f\"ğŸ‰ è®­ç»ƒå®Œæˆï¼ä¿å­˜é€‚é…å™¨åˆ° {OUTPUT_DIR}\")\n",
    "trainer.save_model(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89fb07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# 1. å‡†å¤‡æµ‹è¯•é—®é¢˜ (ç”¨è®­ç»ƒè¿‡çš„é—®é¢˜æµ‹è¯•è¿‡æ‹Ÿåˆï¼Œç”¨æ–°é—®é¢˜æµ‹è¯•æ³›åŒ–)\n",
    "#test_question = \"å¦‚æœä½ æœ‰3ä¸ªè‹¹æœï¼Œåƒæ‰äº†1ä¸ªï¼Œåˆä¹°æ¥äº†5ä¸ªï¼Œç°åœ¨æœ‰å‡ ä¸ªï¼Ÿ\" \n",
    "test_question = \"ä¸‰è§’å½¢çš„åº•æ˜¯8ï¼Œé«˜æ˜¯3ï¼Œæ±‚å…¶é¢ç§¯æ˜¯å¤šå°‘\"\n",
    "# 2. å®šä¹‰æ¨ç†å‡½æ•°\n",
    "def run_inference(model, tokenizer, question):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.1) # ä½æ¸©ä»¥ä¾¿å¤ç°\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n=== âš”ï¸ æ•ˆæœå¯¹å†³ âš”ï¸ ===\\n\")\n",
    "\n",
    "# --- å›åˆ 1: åŸå§‹å‚»ç“œæ¨¡å‹ (Base Student) ---\n",
    "# é‡æ–°åŠ è½½çº¯å‡€æ¨¡å‹\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "print(f\"ğŸ”´ åŸå§‹æ¨¡å‹å›ç­”:\\n{run_inference(base_model, tokenizer, test_question)}\")\n",
    "\n",
    "# --- å›åˆ 2: è’¸é¦åçš„æ¨¡å‹ (Distilled Student) ---\n",
    "# åŠ è½½åˆšæ‰è®­ç»ƒå¥½çš„ LoRA\n",
    "print(\"\\nLoading LoRA adapters...\")\n",
    "distilled_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "print(f\"ğŸŸ¢ è’¸é¦æ¨¡å‹å›ç­”:\\n{run_inference(distilled_model, tokenizer, test_question)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
