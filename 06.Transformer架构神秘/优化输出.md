# 优化输出


1. 之前的输出是从词汇表中选中概率最高的token输出
2. 但我们想让输出更具有创造性和多样性
3. 该进方法: temperature 和 top-k



## temperature预测下一个词的流程

## 回顾[ 贪心解码]

1. 每次预测下一个词， 模型总是选择概率最高的那个词
2. 技术通过torch.argmax函数实现
3. 如果[forword]概率是58%， [toward]概率是34%
4. 模型会拥远选择[forward]
5. 问题：死板,同样的输入总输出同样的结果


## 引入概率采样

1. 根据概率分布进行随机采样
2. 比如轮盘上有几个词， 每个词的扇区大小由它的概率决定
3. 概率越高的词， 扇区越来越大， 被抽中的机会也就越大
4. 但还是有机会抽中其它词
5. 技术上使用torch.multinomial函数替换argmax


# Temperature 的三种模型

1. T=1, logits/T, 这种情况下temperature就不起作用了
2. 多样性是由multinational决定的
3. T>1 时， 相当于将所有的logits值瘦小了
4. 通过softmax计算时， <font color='red'>概率低的token被选中的机会增多</font>
5. T<1 时logits被放大， <font color='red'>概率高的更容易被选中</font>



Temperature总结

优点：

1. 通过Temperature让模型生成文本时更具创造性和多样性
2. 模型会探索哪些概率较低， 但可能更有趣的路径

缺点:

1. 创意过头， 就可能变成混乱
2. 模型可能会生成一些语法不通， 逻辑错误的句子


# Top-k 核心思想


<font color='red'>与其考虑词汇表中几万个词的可能性不如只关注一个小范围、最有可能得候选集</font>


## TOP-k工作原理

1. 设置一个阀值K，如K=5，K=25 ....
2. 从模型的输出(logits)中选择排名前K的哪些token
3. 再从选中的token中，通过multinomial进行抽样


## TOP-K实现步骤

1. 通过torch.topk()从logits中选取前K个token
2. torch.where将所有小于选中token的logits值设置为-inf
3. 重新将更新过的logits送入softmax
4. 哪些被设置为-inf的logits项， 经softmax后全变成了0



