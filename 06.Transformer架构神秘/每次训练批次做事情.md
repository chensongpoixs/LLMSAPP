# 每次训练批次做事情  损失函数 调整 深度学习的参数


## 一、每个批次要做事情

1. optimizer.zero_grad:清除上一个批次留下的旧梯度
2. 计算这个批次训练的<font color='red'>平均损失</font>
3. loss.backward: 根据loss和计算图反向计算梯度
4. optimizer.step:使用新计算的梯度， 更新模型中的参数
5. 打印训练信息



## 二、损失计算


1. 将输入送给模型
2. 再将logits交给cross_entropy计算损失


